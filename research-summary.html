<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Frontier AI Model Capabilities and Training Methods - Research Summary</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: #333;
            background: #f8f9fa;
            padding: 0;
            margin: 0;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
            min-height: 100vh;
        }
        
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            font-weight: 700;
        }
        
        .header .meta {
            font-size: 1.1rem;
            opacity: 0.95;
            margin-top: 1rem;
        }
        
        .header .meta p {
            margin: 0.5rem 0;
        }
        
        .content {
            padding: 3rem 2rem;
        }
        
        h1 {
            font-size: 2.2rem;
            color: #2c3e50;
            margin: 2.5rem 0 1.5rem 0;
            font-weight: 700;
            border-bottom: 3px solid #667eea;
            padding-bottom: 0.5rem;
        }
        
        h2 {
            font-size: 1.8rem;
            color: #34495e;
            margin: 2rem 0 1rem 0;
            font-weight: 600;
            border-left: 4px solid #667eea;
            padding-left: 1rem;
        }
        
        h3 {
            font-size: 1.4rem;
            color: #34495e;
            margin: 1.5rem 0 0.8rem 0;
            font-weight: 600;
        }
        
        h4 {
            font-size: 1.2rem;
            color: #555;
            margin: 1.2rem 0 0.6rem 0;
            font-weight: 600;
        }
        
        p {
            margin: 1rem 0;
            text-align: justify;
        }
        
        .quote {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 1rem 1.5rem;
            margin: 1rem 0;
            border-radius: 4px;
            font-style: italic;
            position: relative;
        }
        
        .quote .citation {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 0.2rem 0.6rem;
            border-radius: 3px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-left: 0.5rem;
            font-style: normal;
            text-decoration: none;
            vertical-align: middle;
        }
        
        .quote .citation:hover {
            background: #5568d3;
            cursor: pointer;
        }
        
        hr {
            border: none;
            border-top: 2px solid #e0e0e0;
            margin: 2rem 0;
        }
        
        .references {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 3px solid #667eea;
        }
        
        .references h2 {
            margin-bottom: 2rem;
        }
        
        .reference-item {
            margin: 1.5rem 0;
            padding: 1.2rem;
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            border-radius: 4px;
            transition: transform 0.2s, box-shadow 0.2s;
            display: flex;
            align-items: flex-start;
        }
        
        .reference-item:hover {
            transform: translateX(5px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .reference-number {
            display: inline-block;
            background: #667eea;
            color: white;
            padding: 0.3rem 0.7rem;
            border-radius: 4px;
            font-weight: 700;
            margin-right: 1rem;
            min-width: 60px;
            text-align: center;
            flex-shrink: 0;
        }
        
        .reference-content {
            flex: 1;
        }
        
        .reference-citation {
            margin-bottom: 0.5rem;
        }
        
        .reference-quote {
            margin-top: 0.8rem;
            padding: 0.8rem;
            background: white;
            border-left: 3px solid #764ba2;
            border-radius: 3px;
            font-style: italic;
            color: #555;
        }
        
        .reference-quote::before {
            content: "Quote: ";
            font-weight: 700;
            font-style: normal;
            color: #764ba2;
        }
        
        .reference-analysis {
            margin-top: 0.8rem;
            padding: 0.8rem;
            background: #f0f4f8;
            border-left: 3px solid #667eea;
            border-radius: 3px;
            color: #555;
        }
        
        .reference-analysis::before {
            content: "Analysis: ";
            font-weight: 700;
            font-style: normal;
            color: #667eea;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 700;
        }
        
        em {
            color: #555;
            font-style: italic;
        }
        
        a {
            color: #667eea;
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s;
        }
        
        a:hover {
            border-bottom-color: #667eea;
        }
        
        ul, ol {
            margin: 1rem 0 1rem 2rem;
        }
        
        li {
            margin: 0.5rem 0;
        }
        
        code {
            background: #f4f4f4;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        .footer {
            background: #2c3e50;
            color: white;
            padding: 2rem;
            text-align: center;
            margin-top: 4rem;
        }
        
        @media print {
            body {
                background: white;
            }
            .container {
                box-shadow: none;
            }
            .header {
                background: #667eea;
            }
        }
        
        @media (max-width: 768px) {
            .content {
                padding: 2rem 1rem;
            }
            .header {
                padding: 2rem 1rem;
            }
            .header h1 {
                font-size: 1.8rem;
            }
            h1 {
                font-size: 1.8rem;
            }
            h2 {
                font-size: 1.5rem;
            }
            h3 {
                font-size: 1.2rem;
            }
            .reference-item {
                flex-direction: column;
            }
            .reference-number {
                margin-bottom: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Executive Summary: Frontier AI Model Capabilities and Training Methods</h1>
            <div class="meta">
                <p><strong>Research Date:</strong> November 14, 2025</p>
                <p><strong>Topic:</strong> Frontier AI Model Capabilities and Training Methods</p>
                <p><strong>Models Covered:</strong> OpenAI GPT-5, OpenAI o-series (o1, o3-mini, o1-preview), Anthropic Claude 3.5 Sonnet / Claude 3.7 Opus, Google Gemini Ultra 2.0, Meta Llama 4 (frontier-scale internal variants), DeepSeek V3 and DeepSeek R1</p>
            </div>
        </div>
        <div class="content">
<h1>Executive Summary: Frontier AI Model Capabilities and Training Methods</h1>

<p><strong>Research Date:</strong> November 14, 2025</p>
<p><strong>Topic:</strong> Frontier AI Model Capabilities and Training Methods</p>
<p><strong>Models Covered:</strong> OpenAI GPT-5, OpenAI o-series (o1, o3-mini, o1-preview), Anthropic Claude 3.5 Sonnet / Claude 3.7 Opus, Google Gemini Ultra 2.0, Meta Llama 4 (frontier-scale internal variants), DeepSeek V3 and DeepSeek R1</p>

<hr>

<h2>Executive Overview</h2>

<p>As stated in the research parameters, &quot;key technical details are intentionally undisclosed or obscured&quot; for major models, making this &quot;the single hardest topic to research and write about on the web today&quot; <span class="citation">[550]</span>. The most difficult information to find includes &quot;exact training datasets and data mixture ratios, token counts, preprocessing pipelines, compute budgets and cluster topologies, optimizer configurations, RLHF methodologies, supervised fine-tuning sources, architecture modifications (MoE routing logic, attention variants, parallelism strategies), internal eval benchmarks, safety red-team results, and alignment techniques&quot; <span class="citation">[551]</span>. &quot;These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot; <span class="citation">[552]</span>. The field &quot;evolves so fast that even partial information becomes outdated within weeks, and widespread speculation, leaks, and misinformation drown out credible analysis&quot; <span class="citation">[553]</span>. &quot;Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware—accurate reporting requires deep multi-domain expertise yet still lacks access to the primary evidence needed for certainty&quot; <span class="citation">[554]</span>.</p>

<hr>

<h2>1. Introduction: The Challenge of Researching Frontier AI Models</h2>

<div class="quote">&quot;These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot; <span class="citation">[555]</span>. The field &quot;evolves so fast that even partial information becomes outdated within weeks, and widespread speculation, leaks, and misinformation drown out credible analysis&quot; <span class="citation">[556]</span>. &quot;Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware—accurate reporting requires deep multi-domain expertise yet still lacks access to the primary evidence needed for certainty&quot; <span class="citation">[557]</span>.</div>

<hr>

<h2>2. OpenAI GPT-5</h2>

<h3>2.1 Capabilities and Performance</h3>

<div class="quote">&quot;We are introducing GPT‑5, our best AI system yet&quot; <span class="citation">[1]</span>. &quot;GPT‑5 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more&quot; <span class="citation">[2]</span>. &quot;It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses&quot; <span class="citation">[3]</span>. &quot;GPT‑5 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPT‑5 pro, a version with extended reasoning for even more comprehensive and accurate answers&quot; <span class="citation">[4]</span>.</div>

<div class="quote">&quot;GPT‑5 not only outperforms previous models on benchmarks and answers questions more quickly, but—most importantly—is more useful for real-world queries&quot; <span class="citation">[5]</span>. &quot;We&#x27;ve made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, while leveling up GPT‑5&#x27;s performance in three of ChatGPT&#x27;s most common uses: writing, coding, and health&quot; <span class="citation">[6]</span>.</div>

<div class="quote">&quot;GPT‑5 is a unified system with a <strong>smart, efficient model</strong> that answers most questions, a <strong>deeper reasoning model</strong> (GPT‑5 thinking) for harder problems, and a <strong>real‑time router</strong> that quickly decides which to use based on conversation type, complexity, tool needs, and your explicit intent (for example, if you say &#x27;think hard about this&#x27; in the prompt)&quot; <span class="citation">[69]</span>. &quot;The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time&quot; <span class="citation">[70]</span>. &quot;Once usage limits are reached, a mini version of each model handles remaining queries&quot; <span class="citation">[71]</span>. &quot;In the near future, we plan to integrate these capabilities into a single model&quot; <span class="citation">[72]</span>.</div>

<div class="quote">&quot;GPT‑5 gets more value out of less thinking time&quot; <span class="citation">[73]</span>. &quot;In our evaluations, GPT‑5 (with thinking) performs better than OpenAI o3 with 50-80% less output tokens across capabilities, including visual reasoning, agentic coding, and graduate-level scientific problem solving&quot; <span class="citation">[74]</span>. &quot;Overall, GPT‑5 is <strong>less effusively agreeable</strong>, uses <strong>fewer unnecessary emojis</strong>, and is more subtle and thoughtful in follow‑ups compared to GPT‑4o&quot; <span class="citation">[75]</span>. &quot;It should feel less like &#x27;talking to AI&#x27; and more like <strong>chatting with a helpful friend</strong> with PhD‑level intelligence&quot; <span class="citation">[76]</span>. &quot;Earlier this year, we <a href="https://openai.com/index/sycophancy-in-gpt-4o/" target="_blank">released an update to GPT‑4o⁠</a> that unintentionally made the model overly sycophantic, or excessively flattering or agreeable&quot; <span class="citation">[77]</span>. &quot;We quickly <a href="https://openai.com/index/expanding-on-sycophancy/" target="_blank">rolled back the change⁠</a> and have since worked to understand and reduce this behavior by: Developing new evaluations to measure sycophancy levels, Improving our training so the model is less sycophantic—for instance, adding examples that would normally lead to over-agreement, and then teaching it not to do that&quot; <span class="citation">[78]</span>. &quot;In targeted sycophancy evaluations using prompts specifically designed to elicit sycophantic responses, GPT‑5 meaningfully reduced sycophantic replies (from 14.5% to less than 6%)&quot; <span class="citation">[79]</span>. &quot;At times, reducing sycophancy can come with reductions in user satisfaction, but the improvements we made cut sycophancy by more than half while also delivering other measurable gains, so users continue to have high-quality, constructive conversations—in line with our goal to <a href="https://openai.com/index/how-we&#x27;re-optimizing-chatgpt/" target="_blank">help people use ChatGPT well⁠</a>&quot; <span class="citation">[80]</span>.</div>

<div class="quote">&quot;GPT‑5 is significantly better at instruction following, and we see a corresponding improvement in its ability to follow custom instructions&quot; <span class="citation">[81]</span>. &quot;We&#x27;re also launching a research preview of four new preset personalities for all ChatGPT users, made possible by the improvements on steerability&quot; <span class="citation">[82]</span>. &quot;These personalities, available initially for text chat and coming later to Voice, let you set how ChatGPT interacts—whether concise and professional, thoughtful and supportive, or a bit sarcastic—without writing custom prompts&quot; <span class="citation">[83]</span>. &quot;The four initial options, Cynic, Robot, Listener, and Nerd, are opt-in, adjustable anytime in settings, and designed to match your communication style&quot; <span class="citation">[84]</span>. &quot;All of these new personalities meet or exceed our bar on internal evals for reducing sycophancy&quot; <span class="citation">[85]</span>. &quot;We look forward to learning and iterating based on early feedback&quot; <span class="citation">[86]</span>.</div>

<div class="quote">&quot;We decided to treat the &#x27;GPT‑5 thinking&#x27; model as High capability in the Biological and Chemical domain, and have implemented strong safeguards to sufficiently minimize the associated risks&quot; <span class="citation">[87]</span>. &quot;We rigorously tested the model with our safety evaluations under our <a href="https://openai.com/index/updating-our-preparedness-framework/" target="_blank">Preparedness Framework⁠⁠</a>, completing 5,000 hours of red-teaming with partners like the CAISI and UK AISI&quot; <span class="citation">[88]</span>. &quot;Similar to our approach for ChatGPT Agent, while we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm–our <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf" target="_blank">defined threshold⁠(opens in a new window)</a> for High capability–we are taking a precautionary approach and are activating the required safeguards now in order to increase readiness for when such capabilities are available&quot; <span class="citation">[89]</span>. &quot;As a result, &#x27;GPT‑5 thinking&#x27; has a robust safety stack with a multilayered defense system for biology: comprehensive threat modeling, training the model to not output harmful content through our new safe completions paradigm, always-on classifiers and reasoning monitors, and clear enforcement pipelines&quot; <span class="citation">[90]</span>. &quot;Read more about our robust safety approach for GPT‑5 in our <a href="https://openai.com/index/gpt-5-system-card/" target="_blank">system card</a>&quot; <span class="citation">[91]</span>.</div>

<div class="quote">&quot;For the most challenging, complex tasks, we are also releasing GPT‑5 pro, replacing OpenAI o3‑pro, a variant of GPT‑5 that thinks for ever longer, using scaled but efficient parallel test-time compute, to provide the highest quality and most comprehensive answers&quot; <span class="citation">[92]</span>. &quot;GPT‑5 pro achieves the highest performance in the GPT‑5 family on several challenging intelligence benchmarks, including state-of-the-art performance on GPQA, which contains extremely difficult science questions&quot; <span class="citation">[93]</span>. &quot;In evaluations on over 1000 economically valuable, real-world reasoning prompts, external experts preferred GPT‑5 pro over &#x27;GPT‑5 thinking&#x27; 67.8% of the time&quot; <span class="citation">[94]</span>. &quot;GPT‑5 pro made 22% fewer major errors and excelled in health, science, mathematics, and coding&quot; <span class="citation">[95]</span>. &quot;Experts rated its responses as relevant, useful, and comprehensive&quot; <span class="citation">[96]</span>.</div>

<div class="quote">&quot;GPT‑5 is the new default in ChatGPT, replacing GPT‑4o, OpenAI o3, OpenAI o4-mini, GPT‑4.1, and GPT‑4.5 for signed-in users&quot; <span class="citation">[97]</span>. &quot;Just open ChatGPT and type your question; GPT‑5 handles the rest <strong>,</strong> applying reasoning automatically when the response would benefit from it&quot; <span class="citation">[98]</span>. &quot;Paid users can still select <strong>&#x27;GPT‑5 Thinking&#x27;</strong> from the model picker, or type something like &#x27;think hard about this&#x27; in the prompt to ensure reasoning is used when generating a response&quot; <span class="citation">[99]</span>. &quot;GPT‑5 is starting to roll out today <strong>to all Plus, Pro, Team, and Free users, with access for Enterprise and Edu coming next week</strong>&quot; <span class="citation">[100]</span>. &quot;Pro, Plus, and Team users can also start coding with GPT‑5 in the <a href="https://github.com/openai/codex" target="_blank">Codex CLI⁠(opens in a new window)</a> by signing in with ChatGPT&quot; <span class="citation">[101]</span>. &quot;As with GPT‑4o, the difference between free and paid access to GPT‑5 is usage volume&quot; <span class="citation">[102]</span>. &quot;Pro subscribers get unlimited access to GPT‑5, and access to <strong>GPT‑5 Pro</strong>&quot; <span class="citation">[103]</span>. &quot;Plus users can use it comfortably as their default model for everyday questions, with significantly higher usage than free users&quot; <span class="citation">[104]</span>. &quot;Team, Enterprise, and Edu customers can also use GPT‑5 comfortably as their default model for everyday work, with generous limits that make it easy for entire organizations to rely on GPT‑5&quot; <span class="citation">[105]</span>. &quot;For ChatGPT free-tier users, full reasoning capabilities may take a few days to fully roll out&quot; <span class="citation">[106]</span>. &quot;Once free users reach their GPT‑5 usage limits, they will transition to <strong>GPT‑5 mini</strong>, a smaller, faster, and highly capable model&quot; <span class="citation">[107]</span>.</div>

<div class="quote">&quot;GPT‑5 is our strongest coding model to date&quot; <span class="citation">[7]</span>. &quot;It shows particular improvements in <strong>complex front‑end generation</strong> and <strong>debugging larger repositories</strong>&quot; <span class="citation">[8]</span>. &quot;It can often create beautiful and responsive websites, apps, and games with an eye for aesthetic sensibility in just one prompt, intuitively and tastefully turning ideas into reality&quot; <span class="citation">[9]</span>. &quot;Early testers also noted its design choices, with a much better understanding of things like spacing, typography, and white space&quot; <span class="citation">[10]</span>.</div>

<div class="quote">&quot;GPT‑5 is our most capable writing collaborator yet, able to help you steer and translate rough ideas into <strong>compelling, resonant writing</strong> with literary depth and rhythm&quot; <span class="citation">[11]</span>. &quot;It more reliably handles writing that involves structural ambiguity, such as sustaining unrhymed iambic pentameter or free verse that flows naturally, combining respect for form with expressive clarity&quot; <span class="citation">[12]</span>.</div>

<div class="quote">&quot;GPT‑5 is our best model yet for health-related questions, empowering users to be informed about and advocate for their health&quot; <span class="citation">[13]</span>. &quot;The model scores significantly higher than any previous model on <a href="https://openai.com/index/healthbench/" target="_blank"><strong>HealthBench</strong> ⁠</a>, an evaluation we published earlier this year based on realistic scenarios and physician-defined criteria&quot; <span class="citation">[14]</span>. &quot;Compared to previous models, it acts more like an active thought partner, proactively flagging potential concerns and asking questions to give more helpful answers&quot; <span class="citation">[15]</span>.</div>

<div class="quote">&quot;GPT‑5 is much smarter across the board, as reflected by its performance on academic and human-evaluated benchmarks, particularly in math, coding, visual perception, and health&quot; <span class="citation">[16]</span>. &quot;It sets a new <strong>state of the art across math (94.6% on AIME 2025 without tools), real-world coding (74.9% on SWE-bench Verified, 88% on Aider Polyglot), multimodal understanding (84.2% on MMMU), and health (46.2% on HealthBench Hard)</strong>—and those gains show up in everyday use&quot; <span class="citation">[17]</span>. &quot;With GPT‑5 pro&#x27;s extended reasoning, the model also sets a new SOTA on <strong>GPQA</strong>, scoring 88.4% without tools&quot; <span class="citation">[18]</span>.</div>

<h3>2.2 Training Process</h3>

<div class="quote">&quot;GPT‑5 was trained on Microsoft Azure AI supercomputers&quot; <span class="citation">[19]</span>. According to Epoch AI research, &quot;GPT-5 used less training compute than GPT-4.5 because OpenAI focused on scaling post-training&quot; <span class="citation">[436]</span>. &quot;New post-training techniques made it possible to outperform GPT-4.5 with less training compute, but these methods likely weren&#x27;t yet mature enough to be applied at GPT-4.5&#x27;s compute scale&quot; <span class="citation">[437]</span>. &quot;Doing so would&#x27;ve taken more time (and compute), which OpenAI likely chose not to do due to strong market pressures&quot; <span class="citation">[438]</span>.</div>

<div class="quote">&quot;Until recently, most LLMs were trained with <a href="https://epoch.ai/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining" target="_blank">100× more pre-training than post-training compute</a>&quot; <span class="citation">[439]</span>. &quot;However, around September 2024, researchers developed novel techniques used in &#x27;reasoning models&#x27; that help scale post-training compute effectively&quot; <span class="citation">[440]</span>. &quot;Researchers could now triple post-training compute in a way that was at least as useful as tripling pre-training compute&quot; <span class="citation">[441]</span>. &quot;In fact, these reasoning techniques make it possible to reduce pre-training compute by <a href="https://epoch.ai/gradient-updates/quantifying-the-algorithmic-improvement-from-reasoning-models" target="_blank">roughly 10×</a> while getting the same performance!&quot; <span class="citation">[442]</span>.</div>

<div class="quote">&quot;Out of all the GPT models, GPT-5 is the odd one out&quot; <span class="citation">[443]</span>. &quot;Unlike all previous versions of GPT, it was likely <a href="https://x.com/EpochAIResearch/status/1953883611389702169" target="_blank">trained on less compute</a> than its immediate predecessor, GPT-4.5&quot; <span class="citation">[444]</span>. &quot;While the exact numbers are uncertain, GPT-4.5 very likely used more training compute than GPT-5&quot; <span class="citation">[445]</span>. &quot;But this leads to a puzzle: Models trained with more compute tend to be better, so why did OpenAI train GPT-5 with less compute than GPT-4.5?&quot; <span class="citation">[446]</span>.</div>

<div class="quote">&quot;Importantly, when we say &#x27;training compute&#x27;, we&#x27;re focusing on the compute to perform the final training run of a model&quot; <span class="citation">[447]</span>. &quot;It&#x27;s likely that the total compute for developing GPT-5 was higher than for GPT-4.5, if we also account for the compute for running experiments&quot; <span class="citation">[448]</span>. &quot;This is because OpenAI&#x27;s (projected) R&amp;D compute spend has grown from <a href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=spkbjw" target="_blank">~$5 billion</a> in 2024 to <a href="https://www.theinformation.com/articles/openai-spend-100-billion-backup-servers-ai-breakthroughs?rc=spkbjw" target="_blank">~$9 billion in 2025</a>&quot; <span class="citation">[449]</span>.</div>

<div class="quote">&quot;Why did GPT-5 use less training compute than GPT-4.5? We believe this is a combination of two factors&quot; <span class="citation">[450]</span>. &quot;First, OpenAI decided to prioritize scaling post-training, which had better returns on the margin&quot; <span class="citation">[451]</span>. &quot;Second, they couldn&#x27;t readily scale post-training compute to GPT-4.5 levels at the time&quot; <span class="citation">[452]</span>. &quot;And if they tried to scale post-training on a model with as much pre-training as GPT-4.5, they would&#x27;ve run into timing and experimental compute constraints&quot; <span class="citation">[453]</span>.</div>

<div class="quote">&quot;This means that, rather than spending around $200 million on pre-training and $2 million on post-training GPT-4.5, new post-training techniques made it possible for that $2 million in post-training to achieve the same overall performance with only $20 million in pre-training&quot; <span class="citation">[454]</span>. &quot;That&#x27;s roughly a ten-fold decrease in training costs, though this doesn&#x27;t imply that total model development costs were lower, due to increases in the compute needed to run experiments&quot; <span class="citation">[455]</span>. &quot;The upshot is that OpenAI was likely able to train a model with less compute than GPT-4.5, while still <a href="https://openai.com/index/introducing-gpt-4-5/" target="_blank">outperforming</a> it on many useful tasks like <a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf" target="_blank">coding and search</a>&quot; <span class="citation">[456]</span>.</div>

<div class="quote">&quot;However, while this shows that OpenAI could&#x27;ve outperformed GPT-4.5 with less training compute, it doesn&#x27;t fully explain why they chose this strategy in practice&quot; <span class="citation">[457]</span>. &quot;For example, why not just post-train GPT-4.5? And why not post-train a smaller model on enough data to reach GPT-4.5&#x27;s level of training compute?&quot; <span class="citation">[458]</span>. &quot;The core reason is that scaling post-training in this way is challenging&quot; <span class="citation">[459]</span>. &quot;It requires lots of testing and experimentation, which takes time and compute, especially when performed on larger, newer models&quot; <span class="citation">[460]</span>. &quot;It also requires a significant amount of high-quality post-training data, which takes time to design and collect&quot; <span class="citation">[461]</span>.</div>

<div class="quote">&quot;Crucially, OpenAI faced major time constraints due to market pressures&quot; <span class="citation">[462]</span>. &quot;This came in the form of fierce competition from rival AI labs, which would hurt their <a href="https://epoch.ai/data-insights/ai-companies-revenue" target="_blank">revenue</a> – e.g. Anthropic&#x27;s models had been <a href="https://epoch.ai/benchmarks/swe-bench-verified" target="_blank">consistently outperforming</a> OpenAI&#x27;s models at coding&quot; <span class="citation">[463]</span>. &quot;And there was added pressure because many had expected OpenAI to release a model called &#x27;GPT-5&#x27; as early as <a href="https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded" target="_blank">November 2023</a>&quot; <span class="citation">[464]</span>.</div>

<div class="quote">&quot;Given these constraints, we believe that OpenAI scaled post-training on a smaller model as much as they could&quot; <span class="citation">[465]</span>. &quot;Scaling further would&#x27;ve either required more experiments than they had the compute or time for, or post-training data that they didn&#x27;t have&quot; <span class="citation">[466]</span>. &quot;Post-training a GPT-4.5-sized model, let alone starting a larger <a href="https://epoch.ai/data-insights/longest-training-run" target="_blank">multi-month</a> pre-training run and doing post-training on top, would&#x27;ve taken too much time or too much experiment compute&quot; <span class="citation">[467]</span>. &quot;The result of these efforts in scaling post-training was GPT-5, a new state-of-the-art model that OpenAI was able to release by August&quot; <span class="citation">[468]</span>.</div>

<div class="quote">&quot;What does this mean for training compute trends moving forward? Our best guess is that future iterations of GPT will be trained on more compute&quot; <span class="citation">[469]</span>. &quot;To see why, consider the bigger picture&quot; <span class="citation">[470]</span>. &quot;Training GPT-5 with less compute than GPT-4.5 is part of a broader trend, where the training compute of state-of-the-art models has grown more slowly than one might&#x27;ve expected a year ago&quot; <span class="citation">[471]</span>. &quot;Since post-training was just a small portion of training compute and scaling it yielded huge returns, AI labs focused their limited training compute on scaling it rather than pre-training&quot; <span class="citation">[472]</span>.</div>

<div class="quote">&quot;In fact, these reasoning post-training techniques have scaled <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale" target="_blank">much faster than pre-training compute</a>&quot; <span class="citation">[473]</span>. &quot;At this rate, tripling post-training compute will soon be akin to tripling the entire compute budget – so current growth rates likely <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale" target="_blank">can&#x27;t be sustained</a> for much more than a year&quot; <span class="citation">[474]</span>. &quot;That means that this broader trend is likely to end – we may see a reversion to the original trend of training compute growth&quot; <span class="citation">[475]</span>. &quot;If this is right, GPT-6 is likely to need much more training compute than GPT-5, and probably more than GPT-4.5&quot; <span class="citation">[476]</span>. &quot;Not to mention, OpenAI plans to significantly expand their compute stock, with many more <a href="https://x.com/sama/status/1947057625780396512?t=OwQjfzYJTdYzy8vRCXaJHg" target="_blank">GPUs brought online by the end of the year</a>, and major clusters like <a href="https://openai.com/index/five-new-stargate-sites/" target="_blank">Stargate Abilene</a> coming out in phases&quot; <span class="citation">[477]</span>.</div>

<h3>2.3 Alignment and Safety</h3>

<div class="quote">&quot;GPT‑5 advances the frontier on safety&quot; <span class="citation">[20]</span>. &quot;In the past, ChatGPT relied primarily on refusal-based safety training: based on the user&#x27;s prompt, the model should either comply or refuse&quot; <span class="citation">[21]</span>. &quot;While this type of training works well for explicitly malicious prompts, it can struggle to handle situations where the user&#x27;s intent is unclear, or information could be used in benign or malicious ways&quot; <span class="citation">[22]</span>. &quot;For GPT‑5, we introduced a new form of safety-training — safe completions — which teaches the model to give the most helpful answer where possible while still staying within safety boundaries&quot; <span class="citation">[23]</span>.</div>

<div class="quote">&quot;GPT‑5 is significantly less likely to hallucinate than our previous models&quot; <span class="citation">[24]</span>. &quot;With web search enabled on anonymized prompts representative of ChatGPT production traffic, GPT‑5&#x27;s responses are ~45% less likely to contain a factual error than GPT‑4o, and when thinking, GPT‑5&#x27;s responses are ~80% less likely to contain a factual error than OpenAI o3&quot; <span class="citation">[25]</span>.</div>

<div class="quote">&quot;GPT‑5 (with thinking) more honestly communicates its actions and capabilities to the user—especially for tasks which are impossible, underspecified, or missing key tools&quot; <span class="citation">[26]</span>. &quot;In order to achieve a high reward during training, reasoning models may learn to lie about successfully completing a task or be overly confident about an uncertain answer&quot; <span class="citation">[27]</span>. &quot;We evaluated deception rates on settings involving impossible coding tasks and missing multimodal assets, and found that GPT‑5 (with thinking) is less deceptive than o3 across the board&quot; <span class="citation">[28]</span>. &quot;On a large set of conversations representative of real production ChatGPT traffic, we&#x27;ve reduced rates of deception from 4.8% for o3 to 2.1% of GPT‑5 reasoning responses&quot; <span class="citation">[29]</span>.</div>

<hr>

<h2>3. OpenAI o-Series Models (o1, o3-mini, o1-preview)</h2>

<h3>3.1 Reasoning Capabilities</h3>

<div class="quote">&quot;OpenAI o1 ranks in the 89th percentile on competitive programming questions (Codeforces), places among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), and exceeds human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA)&quot; <span class="citation">[30]</span>.</div>

<div class="quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot; <span class="citation">[31]</span>. &quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot; <span class="citation">[32]</span>. &quot;A score of 13.9 places it among the top 500 students nationally and above the cutoff for the USA Mathematical Olympiad&quot; <span class="citation">[33]</span>.</div>

<div class="quote">&quot;We also evaluated o1 on GPQA diamond, a difficult intelligence benchmark which tests for expertise in chemistry, physics and biology&quot; <span class="citation">[34]</span>. &quot;In order to compare models to humans, we recruited experts with PhDs to answer GPQA-diamond questions&quot; <span class="citation">[35]</span>. &quot;We found that o1 surpassed the performance of those human experts, becoming the first model to do so on this benchmark&quot; <span class="citation">[36]</span>. &quot;These results do not imply that o1 is more capable than a PhD in all respects — only that the model is more proficient in solving some problems that a PhD would be expected to solve&quot; <span class="citation">[37]</span>.</div>

<h3>3.2 Training Methodology</h3>

<div class="quote">&quot;Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process&quot; <span class="citation">[38]</span>. &quot;We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute)&quot; <span class="citation">[39]</span>. &quot;The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them&quot; <span class="citation">[40]</span>.</div>

<div class="quote">&quot;o1 performance smoothly improves with both train-time and test-time compute&quot; <span class="citation">[108]</span>. &quot;To highlight the reasoning improvement over GPT‑4o, we tested our models on a diverse set of human exams and ML benchmarks&quot; <span class="citation">[109]</span>. &quot;We show that o1 significantly outperforms GPT‑4o on the vast majority of these reasoning-heavy tasks&quot; <span class="citation">[110]</span>. &quot;Unless otherwise specified, we evaluated o1 on the maximal test-time compute setting&quot; <span class="citation">[111]</span>. &quot;o1 greatly improves over GPT-4o on challenging reasoning benchmarks&quot; <span class="citation">[112]</span>. &quot;Solid bars show pass@1 accuracy and the shaded region shows the performance of majority vote (consensus) with 64 samples&quot; <span class="citation">[113]</span>.</div>

<div class="quote">&quot;In many reasoning-heavy benchmarks, o1 rivals the performance of human experts&quot; <span class="citation">[114]</span>. &quot;Recent frontier models do so well on MATH and GSM8K that these benchmarks are no longer effective at differentiating models&quot; <span class="citation">[115]</span>. &quot;We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America&quot; <span class="citation">[116]</span>. &quot;On several other ML benchmarks, o1 improved over the state-of-the-art&quot; <span class="citation">[117]</span>. &quot;With its vision perception capabilities enabled, o1 scored 78.2% on MMMU, making it the first model to be competitive with human experts&quot; <span class="citation">[118]</span>. &quot;It also outperformed GPT‑4o on 54 out of 57 MMLU subcategories&quot; <span class="citation">[119]</span>.</div>

<div class="quote">&quot;In addition to exams and academic benchmarks, we also evaluated human preference of o1‑preview vs GPT‑4o on challenging, open-ended prompts in a broad spectrum of domains&quot; <span class="citation">[120]</span>. &quot;In this evaluation, human trainers were shown anonymized responses to a prompt from o1‑preview and GPT‑4o, and voted for which response they preferred&quot; <span class="citation">[121]</span>. &quot;o1‑preview is preferred to gpt-4o by a large margin in reasoning-heavy categories like data analysis, coding, and math&quot; <span class="citation">[122]</span>. &quot;However, o1‑preview is not preferred on some natural language tasks, suggesting that it is not well-suited for all use cases&quot; <span class="citation">[123]</span>.</div>

<div class="quote">&quot;Chain of thought reasoning provides new opportunities for alignment and safety&quot; <span class="citation">[124]</span>. &quot;We found that integrating our policies for model behavior into the chain of thought of a reasoning model is an effective way to robustly teach human values and principles&quot; <span class="citation">[125]</span>. &quot;By teaching the model our safety rules and how to reason about them in context, we found evidence of reasoning capability directly benefiting model robustness: o1‑preview achieved substantially improved performance on key jailbreak evaluations and our hardest internal benchmarks for evaluating our model&#x27;s safety refusal boundaries&quot; <span class="citation">[126]</span>. &quot;We believe that using a chain of thought offers significant advances for safety and alignment because (1) it enables us to observe the model thinking in a legible way, and (2) the model reasoning about safety rules is more robust to out-of-distribution scenarios&quot; <span class="citation">[127]</span>.</div>

<div class="quote">&quot;To stress-test our improvements, we conducted a suite of safety tests and red-teaming before deployment, in accordance with our <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf" target="_blank">Preparedness Framework⁠(opens in a new window)</a>&quot; <span class="citation">[128]</span>. &quot;We found that chain of thought reasoning contributed to capability improvements across our evaluations&quot; <span class="citation">[129]</span>. &quot;Of particular note, we observed <a href="https://cdn.openai.com/o1-system-card.pdf#page=16" target="_blank">interesting instances of reward hacking⁠(opens in a new window)</a>&quot; <span class="citation">[130]</span>. &quot;Detailed results from these evaluations can be found in the accompanying <a href="https://openai.com/index/openai-o1-system-card/" target="_blank">System Card</a>&quot; <span class="citation">[131]</span>.</div>

<div class="quote">&quot;We believe that a hidden chain of thought presents a unique opportunity for monitoring models&quot; <span class="citation">[132]</span>. &quot;Assuming it is faithful and legible, the hidden chain of thought allows us to &#x27;read the mind&#x27; of the model and understand its thought process&quot; <span class="citation">[133]</span>. &quot;For example, in the future we may wish to monitor the chain of thought for signs of manipulating the user&quot; <span class="citation">[134]</span>. &quot;However, for this to work the model must have freedom to express its thoughts in unaltered form, so we cannot train any policy compliance or user preferences onto the chain of thought&quot; <span class="citation">[135]</span>. &quot;We also do not want to make an unaligned chain of thought directly visible to users&quot; <span class="citation">[136]</span>. &quot;Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users&quot; <span class="citation">[137]</span>. &quot;We acknowledge this decision has disadvantages&quot; <span class="citation">[138]</span>. &quot;We strive to partially make up for it by teaching the model to reproduce any useful ideas from the chain of thought in the answer&quot; <span class="citation">[139]</span>. &quot;For the o1 model series we show a model-generated summary of the chain of thought&quot; <span class="citation">[140]</span>.</div>

<div class="quote">&quot;We trained a model that scored 213 points and ranked in the 49th percentile in the 2024 International Olympiad in Informatics (IOI), by initializing from o1 and training to further improve programming skills&quot; <span class="citation">[141]</span>. &quot;This model competed in the 2024 IOI under the same conditions as the human contestants&quot; <span class="citation">[142]</span>. &quot;It had ten hours to solve six challenging algorithmic problems and was allowed 50 submissions per problem&quot; <span class="citation">[143]</span>. &quot;For each problem, our system sampled many candidate submissions and submitted 50 of them based on a test-time selection strategy&quot; <span class="citation">[144]</span>. &quot;Submissions were selected based on performance on the IOI public test cases, model-generated test cases, and a learned scoring function&quot; <span class="citation">[145]</span>. &quot;If we had instead submitted at random, we would have only scored 156 points on average, suggesting that this strategy was worth nearly 60 points under competition constraints&quot; <span class="citation">[146]</span>. &quot;With a relaxed submission constraint, we found that model performance improved significantly&quot; <span class="citation">[147]</span>. &quot;When allowed 10,000 submissions per problem, the model achieved a score of 362.14 – above the gold medal threshold – even without any test-time selection strategy&quot; <span class="citation">[148]</span>. &quot;Finally, we simulated competitive programming contests hosted by Codeforces to demonstrate this model&#x27;s coding skill&quot; <span class="citation">[149]</span>. &quot;Our evaluations closely matched competition rules and allowed for 10 submissions&quot; <span class="citation">[150]</span>. &quot;GPT‑4o achieved an Elo rating of 808, which is in the 11th percentile of human competitors&quot; <span class="citation">[151]</span>. &quot;This model far exceeded both GPT‑4o and o1—it achieved an Elo rating of 1807, performing better than 93% of competitors&quot; <span class="citation">[152]</span>. &quot;Further fine-tuning on programming competitions improves o1&quot; <span class="citation">[153]</span>. &quot;The improved model ranked in the 49th percentile in the 2024 International Olympiad in Informatics under competition rules&quot; <span class="citation">[154]</span>.</div>

<div class="quote">&quot;o1 significantly advances the state-of-the-art in AI reasoning&quot; <span class="citation">[155]</span>. &quot;We plan to release improved versions of this model as we continue iterating&quot; <span class="citation">[156]</span>. &quot;We expect these new reasoning capabilities will improve our ability to align models to human values and principles&quot; <span class="citation">[157]</span>. &quot;We believe o1 – and its successors – will unlock many new use cases for AI in science, coding, math, and related fields&quot; <span class="citation">[158]</span>. &quot;We are excited for users and API developers to discover how it can improve their daily work&quot; <span class="citation">[159]</span>.</div>

<h3>3.3 Chain of Thought Reasoning</h3>

<div class="quote">&quot;Similar to how a human may think for a long time before responding to a difficult question, o1 uses a chain of thought when attempting to solve a problem&quot; <span class="citation">[41]</span>. &quot;Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses&quot; <span class="citation">[42]</span>. &quot;It learns to recognize and correct its mistakes&quot; <span class="citation">[43]</span>. &quot;It learns to break down tricky steps into simpler ones&quot; <span class="citation">[44]</span>. &quot;It learns to try a different approach when the current one isn&#x27;t working&quot; <span class="citation">[45]</span>. &quot;This process dramatically improves the model&#x27;s ability to reason&quot; <span class="citation">[46]</span>.</div>

<h3>3.4 OpenAI o3 Model</h3>

<div class="quote">&quot;Today, OpenAI previewed their o3 model continuing their recent progress on training language models to reason with o1&quot; <span class="citation">[478]</span>. &quot;These models, starting with o3-mini, are expected to be available to the general public in late January of 2025&quot; <span class="citation">[479]</span>. &quot;There was no moment with a &#x27;<a href="https://www.interconnects.ai/p/gpt4-review" target="_blank">GPT-4 release</a>&#x27; level of excitement in 2024&quot; <span class="citation">[480]</span>. &quot;o3 changes that by being far more unexpected than o1, and signals rapid progress across reasoning models&quot; <span class="citation">[481]</span>. &quot;We knew o1 was coming with the long lead-up — the quick and effective follow-up with o3 sets us up for a very dynamic 2025&quot; <span class="citation">[482]</span>.</div>

<div class="quote">&quot;OpenAI&#x27;s o3 shows the industry is beginning to climb its next hill as progress from pretraining only on internet text yields fewer profitable benefits&quot; <span class="citation">[483]</span>. &quot;o3 is a major step change in reasoning evaluations — in summary, it is: The first model to <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough" target="_blank">surpass the 85% threshold for completing the ARC AGI prize</a> (Note: this was done on the public set, not the test set, and exceeded cost constraints)&quot; <span class="citation">[484]</span>. &quot;A step change in state-of-the-art performance on the extremely new <a href="https://epoch.ai/frontiermath" target="_blank">Frontier Math</a> benchmark from 2 to 25%&quot; <span class="citation">[485]</span>. &quot;Substantial improvements were made to all of the leading coding benchmarks, such as SWE-Bench-Verified&quot; <span class="citation">[486]</span>.</div>

<div class="quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot; <span class="citation">[67]</span>. &quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot; <span class="citation">[68]</span>. &quot;Before the o1-class models, OpenAI&#x27;s best model, GPT-4o, only achieved 5% accuracy&quot; <span class="citation">[487]</span>. &quot;The incredible pace of progress on the evaluation as OpenAI hillclimbed on their new reasoning models was <a href="https://x.com/mikeknoop/status/1870172132136931512" target="_blank">summarized by co-founder of ARC Prize Mike Knoop</a>: GPT-2 (2019): 0%, GPT-3 (2020): 0%, GPT-4 (2023): 2%, GPT-4o (2024): 5%, o1-preview (2024): 21%, o1 high (2024): 32%, o1 Pro (2024): ~50%, o3 tuned low (2024): 76%, o3 tuned high (2024): 87%&quot; <span class="citation">[488]</span>.</div>

<div class="quote">&quot;Just in June, the narrative was still that <a href="https://x.com/fchollet/status/1804236584432398622" target="_blank">solving ARC-AGI would be extremely hard</a>&quot; <span class="citation">[489]</span>. &quot;This has totally flipped on its head in just a few months&quot; <span class="citation">[490]</span>. &quot;Even those bullish about rumors of Q* and other reasoning approaches would not have expected this level of success&quot; <span class="citation">[491]</span>. &quot;We tested o3 against two ARC-AGI datasets: Semi-Private Eval: 100 private tasks used to assess overfitting, Public Eval: 400 public tasks&quot; <span class="citation">[492]</span>. &quot;At OpenAI&#x27;s direction, we tested at two levels of compute with variable <strong>sample sizes</strong>: 6 (high-efficiency) and 1024 (low-efficiency, 172x compute)&quot; <span class="citation">[493]</span>.</div>

<div class="quote">&quot;According to <a href="https://semianalysis.com/2024/12/11/scaling-laws-o1-pro-architecture-reasoning-training-infrastructure-orion-and-claude-3-5-opus-failures/" target="_blank">SemiAnalysis</a>, o1 pro uses self-consistency methods or simple consensus@N checks to increase performance by selecting the most common answer across multiple parallel responses to the same query&quot; <span class="citation">[494]</span>. &quot;Here, <strong>sample size</strong>, N, likely corresponds to a consensus@N number, indicating that o3 was evaluated in something close to the configuration for o1 pro that customers can use, 6x compute, and a super high configuration with 1024x compute per problem&quot; <span class="citation">[495]</span>. &quot;This scale of inference is not going to be served to standard paid users for a long time&quot; <span class="citation">[496]</span>. &quot;Most users will be exposed to one pass to consensus@10 depending on the specifications of the &#x27;pro&#x27; tier of o1 models&quot; <span class="citation">[497]</span>.</div>

<div class="quote">&quot;The story in deep learning that has been driving progress in the last few years is finding a rich area of potential and hill climbing on it&quot; <span class="citation">[498]</span>. &quot;The first wave of progress was in internet-scale pretraining&quot; <span class="citation">[499]</span>. &quot;Now, OpenAI has identified a hill to climb by scaling reinforcement learning training and long-context reasoning&quot; <span class="citation">[500]</span>. &quot;Given that o3 is only about <a href="https://x.com/_jasonwei/status/1870184982007644614" target="_blank">three months</a> after the <a href="https://www.interconnects.ai/p/reverse-engineering-openai-o1" target="_blank">release of OpenAI&#x27;s o1</a>, the simplest explanation is that it is the same architecture and training methodology, scaled up&quot; <span class="citation">[501]</span>. &quot;There is no evidence other than hearsay that o3 made an architectural change to inference by adding tree search&quot; <span class="citation">[502]</span>. &quot;A core rule of <a href="https://www.interconnects.ai/i/148388607/inference-scaling-laws" target="_blank">inference scaling laws</a> is that sampling more from the same single-stream generation can give performance improvements&quot; <span class="citation">[503]</span>.</div>

<div class="quote">&quot;At the same time, OpenAI released a <a href="https://openai.com/index/deliberative-alignment/" target="_blank">blog post</a> and research <a href="https://assets.ctfassets.net/kftzwdyauwt9/4pNYAZteAQXWtloDdANQ7L/32db347774f3b8e43a028cc98b24e416/OpenAI_Deliberative-Alignment-Reasoning-Enables-Safer_Language-Models_122024_2.pdf" target="_blank">paper</a> on deliberative alignment, showcasing how o1-class models can enhance safety and alignment research&quot; <span class="citation">[504]</span>. &quot;This provides some of the first positive pieces of evidence for the much bigger open question I hinted at earlier: Can enhanced reasoning abilities deliver value outside of verifiable domains?&quot; <span class="citation">[505]</span>. &quot;This will be revisited many times in 2025&quot; <span class="citation">[506]</span>.</div>

<hr>

<h2>4. Anthropic Claude 3.5 Sonnet and Claude 3.7 Opus</h2>

<h3>4.1 Claude 3.5 Sonnet Capabilities</h3>

<div class="quote">&quot;Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet&quot; <span class="citation">[160]</span>. &quot;Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval)&quot; <span class="citation">[161]</span>.</div>

<h3>4.2 Performance Improvements</h3>

<p>Claude 3.5 Sonnet shows &quot;marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone&quot; <span class="citation">[162]</span>. &quot;Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus&quot; <span class="citation">[163]</span>, making it &quot;ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows&quot; <span class="citation">[164]</span>.</p>

<h3>4.3 Coding Capabilities</h3>

<div class="quote">&quot;In an <a href="https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf" target="_blank">internal agentic coding evaluation</a>, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%&quot; <span class="citation">[165]</span>. &quot;Our evaluation tests the model&#x27;s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement&quot; <span class="citation">[166]</span>. &quot;When instructed and <a href="https://www.anthropic.com/news/tool-use-ga" target="_blank">provided with the relevant tools</a>, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities&quot; <span class="citation">[167]</span>.</div>

<h3>4.4 Claude 3.7 Sonnet</h3>

<div class="quote">&quot;Today, we&#x27;re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market&quot; <span class="citation">[168]</span>. &quot;Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made <a href="https://youtu.be/t3nnDXa81Hs" target="_blank">visible to the user</a>&quot; <span class="citation">[169]</span>. &quot;API users also have fine-grained control over _how long_ the model can think for&quot; <span class="citation">[170]</span>.</div>

<div class="quote">&quot;We&#x27;ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market&quot; <span class="citation">[171]</span>. &quot;Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely&quot; <span class="citation">[172]</span>. &quot;This unified approach also creates a more seamless experience for users&quot; <span class="citation">[173]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to <a href="https://www.anthropic.com/research/visible-extended-thinking" target="_blank">think longer before answering</a>&quot; <span class="citation">[174]</span>. &quot;In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet&quot; <span class="citation">[175]</span>. &quot;In <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking" target="_blank">extended thinking mode</a>, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks&quot; <span class="citation">[176]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development&quot; <span class="citation">[186]</span>. &quot;Along with the model, we&#x27;re also introducing a command line tool for agentic coding, Claude Code&quot; <span class="citation">[187]</span>. &quot;Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal&quot; <span class="citation">[188]</span>. &quot;Claude 3.7 Sonnet is now available on all <a href="https://claude.ai/redirect/website.v1.790f2a5b-c1f3-476a-b7c8-3a83a41660c8/new" target="_blank">Claude</a> plans—including Free, Pro, Team, and Enterprise—as well as the <a href="https://docs.claude.com/en/docs/about-claude/models" target="_blank">Claude Developer Platform</a>, <a href="https://aws.amazon.com/bedrock/claude/" target="_blank">Amazon Bedrock</a>, and <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude" target="_blank">Google Cloud&#x27;s Vertex AI</a>&quot; <span class="citation">[189]</span>. &quot;Extended thinking mode is available on all surfaces except the free Claude tier&quot; <span class="citation">[190]</span>. &quot;In both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens&quot; <span class="citation">[191]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet: Frontier reasoning made practical&quot; <span class="citation">[192]</span>. &quot;In developing our reasoning models, we&#x27;ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs&quot; <span class="citation">[193]</span>. &quot;<a href="https://www.anthropic.com/claude/sonnet#customer-stories" target="_blank">Early testing</a> demonstrated Claude&#x27;s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use&quot; <span class="citation">[194]</span>. &quot;Cognition found it far better than any other model at planning code changes and handling full-stack updates&quot; <span class="citation">[195]</span>. &quot;Vercel highlighted Claude&#x27;s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall&quot; <span class="citation">[196]</span>. &quot;In Canva&#x27;s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors&quot; <span class="citation">[197]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models&#x27; ability to solve real-world software issues&quot; <span class="citation">[198]</span>. &quot;Claude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions&quot; <span class="citation">[199]</span>. &quot;Claude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science&quot; <span class="citation">[200]</span>. &quot;Beyond traditional benchmarks, it even outperformed all previous models in our <a href="https://www.anthropic.com/research/visible-extended-thinking" target="_blank">Pokémon gameplay tests</a>&quot; <span class="citation">[201]</span>.</div>

<div class="quote">&quot;Since June 2024, Sonnet has been the preferred model for developers worldwide&quot; <span class="citation">[202]</span>. &quot;Today, we&#x27;re empowering developers further by introducing <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code" target="_blank">Claude Code</a>—our first agentic coding tool—in a limited research preview&quot; <span class="citation">[203]</span>. &quot;Claude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step&quot; <span class="citation">[204]</span>. &quot;Claude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring&quot; <span class="citation">[205]</span>. &quot;In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead&quot; <span class="citation">[206]</span>. &quot;In the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude&#x27;s own understanding of its capabilities&quot; <span class="citation">[207]</span>. &quot;Our goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements&quot; <span class="citation">[208]</span>. &quot;By <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview#install-and-authenticate" target="_blank">joining this preview</a>, you&#x27;ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future&quot; <span class="citation">[209]</span>.</div>

<div class="quote">&quot;We&#x27;ve also improved the coding experience on Claude.ai&quot; <span class="citation">[210]</span>. &quot;Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude&quot; <span class="citation">[211]</span>. &quot;Claude 3.7 Sonnet is our best coding model to date&quot; <span class="citation">[212]</span>. &quot;With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects&quot; <span class="citation">[213]</span>.</div>

<div class="quote">&quot;We&#x27;ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability&quot; <span class="citation">[214]</span>. &quot;The <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">system card</a> for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work&quot; <span class="citation">[215]</span>. &quot;The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them&quot; <span class="citation">[216]</span>. &quot;Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable&quot; <span class="citation">[217]</span>. &quot;Read the full <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">system card</a> to learn more&quot; <span class="citation">[218]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities&quot; <span class="citation">[219]</span>. &quot;With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what <a href="https://darioamodei.com/machines-of-loving-grace" target="_blank">humans can achieve</a>&quot; <span class="citation">[220]</span>.</div>

<h3>4.5 Safety and Alignment</h3>

<div class="quote">&quot;Despite Claude 3.5 Sonnet&#x27;s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at ASL-2&quot; <span class="citation">[177]</span>. &quot;We&#x27;ve engaged with external experts to test and refine the safety mechanisms within this latest model&quot; <span class="citation">[178]</span>, including providing &quot;Claude 3.5 Sonnet to the UK&#x27;s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation&quot; <span class="citation">[179]</span>.</div>

<div class="quote">&quot;Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">unnecessary refusals by 45%</a> compared to its predecessor&quot; <span class="citation">[180]</span>.</div>

<hr>

<h2>5. Google Gemini Ultra 2.0</h2>

<h3>5.1 Model Family Overview</h3>

<div class="quote">&quot;Gemini 2.5 is our most intelligent AI model, capable of reasoning through its thoughts before responding, resulting in enhanced performance and improved accuracy&quot; <span class="citation">[221]</span>. The model family includes &quot;Gemini 2.5 Pro,&quot; described as &quot;Best for coding and highly complex tasks&quot; <span class="citation">[222]</span>, and &quot;Gemini 2.5 Flash,&quot; optimized for &quot;fast performance on everyday tasks&quot; <span class="citation">[223]</span>.</div>

<h3>5.2 Deep Think Capabilities</h3>

<p>Gemini 2.5 features &quot;Deep Think,&quot; described as &quot;an enhanced reasoning mode that uses cutting edge research techniques in parallel thinking and reinforcement learning to significantly improve Gemini&#x27;s ability to solve complex problems&quot; <span class="citation">[224]</span>. Deep Think &quot;can better help tackle problems that require creativity, strategic planning, and making improvements step-by-step&quot; <span class="citation">[225]</span>.</p>

<div class="quote">&quot;Google AI Ultra subscribers, you now have access to Deep Think in the Gemini app&quot; <span class="citation">[530]</span>. &quot;This tool uses parallel thinking to solve complex problems and excels in areas like coding and scientific discovery&quot; <span class="citation">[531]</span>. &quot;You can access Deep Think by toggling it on in the prompt bar within the Gemini app&quot; <span class="citation">[532]</span>. &quot;This new release incorporates feedback from early trusted testers and research breakthroughs&quot; <span class="citation">[533]</span>. &quot;It&#x27;s a significant improvement over what was first <a href="https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#deep-think" target="_blank">announced at I/O</a>, as measured in terms of key benchmark improvements and trusted tester feedback&quot; <span class="citation">[534]</span>. &quot;It is a variation of the model that <a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/" target="_blank">recently achieved</a> the gold-medal standard at this year&#x27;s International Mathematical Olympiad (IMO)&quot; <span class="citation">[535]</span>. &quot;While that model takes hours to reason about complex math problems, today&#x27;s release is faster and more usable day-to-day, while still reaching Bronze-level performance on the 2025 IMO benchmark, based on internal evaluations&quot; <span class="citation">[536]</span>.</div>

<div class="quote">&quot;Deep Think pushes the frontier of thinking capabilities by using parallel thinking techniques&quot; <span class="citation">[537]</span>. &quot;This approach lets Gemini generate many ideas at once and consider them simultaneously, even revising or combining different ideas over time, before arriving at the best answer&quot; <span class="citation">[538]</span>. &quot;Moreover, by extending the inference time or &#x27;thinking time,&#x27; we give Gemini more time to explore different hypotheses, and arrive at creative solutions to complex problems&quot; <span class="citation">[539]</span>. &quot;We&#x27;ve also developed novel reinforcement learning techniques that encourage the model to make use of these extended reasoning paths, thus enabling Deep Think to become a better, more intuitive problem-solver over time&quot; <span class="citation">[540]</span>.</div>

<div class="quote">&quot;Deep Think&#x27;s performance is also reflected in challenging benchmarks that measure coding, science, knowledge and reasoning capabilities&quot; <span class="citation">[541]</span>. &quot;For example, compared to other models without tool use, Gemini 2.5 Deep Think achieves state-of-the-art performance across LiveCodeBench V6, which measures competitive code performance, and Humanity&#x27;s Last Exam, a challenging benchmark that measures expertise in different domains, including science and math&quot; <span class="citation">[542]</span>. &quot;Deep Think in the Gemini app uses parallel thinking techniques to deliver more detailed, creative and thoughtful responses&quot; <span class="citation">[543]</span>. This includes &quot;Iterative development and design&quot; where &quot;Deep Think can improve both the aesthetics and functionality of web development tasks&quot; <span class="citation">[544]</span>. It also excels in &quot;Scientific and mathematical discovery&quot; and &quot;Algorithmic development and code&quot; <span class="citation">[545]</span>.</div>

<div class="quote">&quot;We continue to build safety and responsibility into Gemini throughout the training and deployment lifecycle&quot; <span class="citation">[546]</span>. &quot;In testing, Gemini 2.5 Deep Think demonstrated improved content safety and tone-objectivity compared to Gemini 2.5 Pro, but did have a higher tendency to refuse benign requests&quot; <span class="citation">[547]</span>. &quot;As Gemini&#x27;s problem-solving abilities advance, we are taking a deeper look at risks that come with increased complexity, including our frontier safety evaluations and the implementation of planned mitigations for critical capability levels&quot; <span class="citation">[548]</span>. &quot;Further details on the safety outcomes of Gemini 2.5 Deep Think are available in the <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf" target="_blank">model card</a>&quot; <span class="citation">[549]</span>.</div>

<h3>5.3 Performance Benchmarks</h3>

<p>According to Google&#x27;s performance table, &quot;Gemini 2.5 Pro Thinking&quot; achieves &quot;21.6%&quot; on &quot;Humanity&#x27;s Last Exam (no tools)&quot; <span class="citation">[226]</span>, &quot;86.4%&quot; on &quot;GPQA diamond&quot; <span class="citation">[227]</span>, &quot;88.0%&quot; on &quot;AIME 2025&quot; <span class="citation">[228]</span>, and &quot;69.0%&quot; on &quot;LiveCodeBench (UI: 1/1/2025-5/1/2025)&quot; <span class="citation">[229]</span>.</p>

<h3>5.4 Multimodal Capabilities</h3>

<div class="quote">&quot;Gemini 2.5 builds on the best of Gemini — with native multimodality and a long context window&quot; <span class="citation">[230]</span>. The models support &quot;text, image and video modalities&quot; <span class="citation">[231]</span>, with pricing reflecting these capabilities.</div>

<hr>

<h2>6. Meta Llama 4</h2>

<h3>6.1 Frontier-Scale Variants</h3>

<div class="quote">&quot;We&#x27;re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture&quot; <span class="citation">[233]</span>. &quot;Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU&quot; <span class="citation">[234]</span>. &quot;Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks&quot; <span class="citation">[235]</span>.</div>

<div class="quote">&quot;Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters&quot; <span class="citation">[236]</span>. &quot;Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on <a href="https://lmarena.ai/leaderboard" target="_blank">LMArena</a>&quot; <span class="citation">[237]</span>.</div>

<div class="quote">&quot;These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world&#x27;s smartest LLMs&quot; <span class="citation">[238]</span>. &quot;Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks&quot; <span class="citation">[239]</span>. &quot;Llama 4 Behemoth is still training, and we&#x27;re excited to share more details about it even while it&#x27;s still in flight&quot; <span class="citation">[240]</span>.</div>

<h3>6.2 Training Architecture</h3>

<div class="quote">&quot;Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture&quot; <span class="citation">[241]</span>. &quot;In MoE models, a single token activates only a fraction of the total parameters&quot; <span class="citation">[242]</span>. &quot;MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model&quot; <span class="citation">[243]</span>.</div>

<div class="quote">&quot;As an example, Llama 4 Maverick models have 17B active parameters and 400B total parameters&quot; <span class="citation">[244]</span>. &quot;We use alternating dense and mixture-of-experts (MoE) layers for inference efficiency&quot; <span class="citation">[245]</span>. &quot;MoE layers use 128 routed experts and a shared expert&quot; <span class="citation">[246]</span>. &quot;Each token is sent to the shared expert and also to one of the 128 routed experts&quot; <span class="citation">[247]</span>. &quot;As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models&quot; <span class="citation">[248]</span>. &quot;This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency&quot; <span class="citation">[249]</span>.</div>

<div class="quote">&quot;Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified model backbone&quot; <span class="citation">[250]</span>. &quot;Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data&quot; <span class="citation">[251]</span>. &quot;We also improved the vision encoder in Llama 4&quot; <span class="citation">[252]</span>. &quot;This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM&quot; <span class="citation">[253]</span>.</div>

<div class="quote">&quot;The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets&quot; <span class="citation">[254]</span>.</div>

<h3>6.3 Training Methodology</h3>

<div class="quote">&quot;We developed a new training technique which we refer to as MetaP that allows us to reliably set critical model hyper-parameters such as per-layer learning rates and initialization scales&quot; <span class="citation">[258]</span>. &quot;We found that chosen hyper-parameters transfer well across different values of batch size, model width, depth, and training tokens&quot; <span class="citation">[259]</span>. &quot;Llama 4 enables open source fine-tuning efforts by pre-training on 200 languages, including over 100 with over 1 billion tokens each, and overall 10x more multilingual tokens than Llama 3&quot; <span class="citation">[260]</span>.</div>

<div class="quote">&quot;Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization—while pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU&quot; <span class="citation">[261]</span>. &quot;We continued training the model in what we call &#x27;mid-training&#x27; to improve core capabilities with new training recipes including long context extension using specialized datasets&quot; <span class="citation">[262]</span>. &quot;This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout&quot; <span class="citation">[263]</span>.</div>

<div class="quote">&quot;For mixing modalities, we came up with a carefully curated curriculum strategy that does not trade-off performance compared to the individual modality expert models&quot; <span class="citation">[264]</span>. &quot;With Llama 4, we revamped our post-training pipeline by adopting a different approach: lightweight supervised fine-tuning (SFT) &gt; online reinforcement learning (RL) &gt; lightweight direct preference optimization (DPO)&quot; <span class="citation">[265]</span>. &quot;A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains&quot; <span class="citation">[266]</span>. &quot;To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set&quot; <span class="citation">[267]</span>. &quot;In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance&quot; <span class="citation">[268]</span>. &quot;Furthermore, we implemented a continuous online RL strategy, where we alternated between training the model and then using it to continually filter and retain only medium-to-hard difficulty prompts&quot; <span class="citation">[269]</span>. &quot;This strategy proved highly beneficial in terms of compute and accuracy tradeoffs&quot; <span class="citation">[270]</span>. &quot;We then did a lightweight DPO to handle corner cases related to model response quality, effectively achieving a good balance between the model&#x27;s intelligence and conversational abilities&quot; <span class="citation">[271]</span>. &quot;Both the pipeline architecture and the continuous online RL strategy with adaptive data filtering culminated in an industry-leading, general-purpose chat model with state-of-the-art intelligence and image understanding capabilities&quot; <span class="citation">[272]</span>.</div>

<div class="quote">&quot;Llama 4 Scout is both pre-trained and post-trained with a 256K context length, which empowers the base model with advanced length generalization capability&quot; <span class="citation">[273]</span>. &quot;We present compelling results in tasks such as retrieval with &#x27;retrieval needle in haystack&#x27; for text as well as cumulative negative log-likelihoods (NLLs) over 10 million tokens of code&quot; <span class="citation">[274]</span>. &quot;A key innovation in the Llama 4 architecture is the use of interleaved attention layers <a href="https://arxiv.org/abs/2305.19466" target="_blank">without positional embeddings</a>&quot; <span class="citation">[275]</span>. &quot;Additionally, we employ <a href="https://arxiv.org/pdf/2501.19399" target="_blank">inference time temperature scaling</a> of attention to enhance length generalization&quot; <span class="citation">[276]</span>. &quot;We call this the iRoPE architecture, where &#x27;i&#x27; stands for &#x27;interleaved&#x27; attention layers, highlighting the long-term goal of supporting &#x27;infinite&#x27; context length, and &#x27;RoPE&#x27; refers to the <a href="https://arxiv.org/abs/2104.09864" target="_blank">rotary position embeddings</a> employed in most layers&quot; <span class="citation">[277]</span>.</div>

<div class="quote">&quot;We trained both of our models on a wide variety of image and video frame stills in order to give them broad visual understanding, including of temporal activities and related images&quot; <span class="citation">[278]</span>. &quot;This enables effortless interaction on multi-image inputs alongside text prompts for visual reasoning and understanding tasks&quot; <span class="citation">[279]</span>. &quot;The models were pre-trained on up to 48 images, and we&#x27;ve tested in post-training with good results up to eight images&quot; <span class="citation">[280]</span>. &quot;Llama 4 Scout is also best-in-class on image grounding, able to align user prompts with relevant visual concepts and anchor model responses to regions in the image&quot; <span class="citation">[281]</span>. &quot;This enables more precise visual question answering for the LLM to better understand user intent and localize objects of interest&quot; <span class="citation">[282]</span>. &quot;Llama 4 Scout also exceeds comparable models on coding, reasoning, long context, and image benchmarks and offers stronger performance than all previous Llama models&quot; <span class="citation">[283]</span>.</div>

<div class="quote">&quot;We&#x27;re excited to share a preview of Llama 4 Behemoth, a teacher model that demonstrates advanced intelligence among models in its class&quot; <span class="citation">[284]</span>. &quot;Llama 4 Behemoth is also a multimodal mixture-of-experts model, with 288B active parameters, 16 experts, and nearly two trillion total parameters&quot; <span class="citation">[285]</span>. &quot;Offering state-of-the-art performance for non-reasoning models on math, multilinguality, and image benchmarks, it was the perfect choice to teach the smaller Llama 4 models&quot; <span class="citation">[286]</span>. &quot;We codistilled the Llama 4 Maverick model from Llama 4 Behemoth as a teacher model, resulting in substantial quality improvements across end task evaluation metrics&quot; <span class="citation">[287]</span>. &quot;We developed a novel distillation loss function that dynamically weights the soft and hard targets through training&quot; <span class="citation">[288]</span>. &quot;Codistillation from Llama 4 Behemoth during pre-training amortizes the computational cost of resource-intensive forward passes needed to compute the targets for distillation for the majority of the training data used in student training&quot; <span class="citation">[289]</span>. &quot;For additional new data incorporated in student training, we ran forward passes on the Behemoth model to create distillation targets&quot; <span class="citation">[290]</span>.</div>

<div class="quote">&quot;Post-training a model with two trillion parameters was a significant challenge too that required us to completely overhaul and revamp the recipe, starting from the scale of data&quot; <span class="citation">[291]</span>. &quot;In order to maximize performance, we had to prune 95% of the SFT data, as opposed to 50% for smaller models, to achieve the necessary focus on quality and efficiency&quot; <span class="citation">[292]</span>. &quot;We also found that doing lightweight SFT followed by large-scale reinforcement learning (RL) produced even more significant improvements in reasoning and coding abilities of the model&quot; <span class="citation">[293]</span>. &quot;Our RL recipe focused on sampling hard prompts by doing pass@k analysis with the policy model and crafting a training curriculum of increasing prompt hardness&quot; <span class="citation">[294]</span>. &quot;We also found that dynamically filtering out prompts with zero advantage during training and constructing training batches with mixed prompts from multiple capabilities were instrumental in providing a performance boost on math, reasoning, and coding&quot; <span class="citation">[295]</span>. &quot;Finally, sampling from a variety of system instructions was crucial in ensuring that the model retained its instruction following ability for reasoning and coding and was able to perform well across a variety of tasks&quot; <span class="citation">[296]</span>.</div>

<div class="quote">&quot;Scaling RL for a two trillion parameter model also required revamping our underlying RL infrastructure due to its unprecedented scale&quot; <span class="citation">[297]</span>. &quot;We optimized the design of our MoE parallelization for speed, which enabled faster iteration&quot; <span class="citation">[298]</span>. &quot;We developed a fully asynchronous online RL training framework that enhanced flexibility&quot; <span class="citation">[299]</span>. &quot;Compared to the existing distributed training framework, which sacrifices the compute memory in order to stack all models in memory, our new infrastructure enabled flexible allocation of different models to separate GPUs, balancing resources across multiple models based on computational speed&quot; <span class="citation">[300]</span>. &quot;This innovation resulted in a ~10x improvement in training efficiency over previous generations&quot; <span class="citation">[301]</span>.</div>

<hr>

<h2>7. DeepSeek V3 and DeepSeek R1</h2>

<h3>7.1 DeepSeek V3 Capabilities</h3>

<p>DeepSeek has developed significant training infrastructure, with references to &quot;自研训练框架、自建智算集群和万卡算力&quot; (self-developed training framework, self-built intelligent computing cluster, and 10,000-card computing power) <span class="citation">[302]</span>. The company has achieved rapid development, with &quot;仅用半年时间便已发布并开源多个百亿级参数大模型&quot; (releasing and open-sourcing multiple 10-billion-parameter large models in just half a year) <span class="citation">[303]</span>.</p>

<h3>7.2 DeepSeek R1 Reasoning Model</h3>

<div class="quote">&quot;We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1&quot; <span class="citation">[304]</span>. &quot;DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities&quot; <span class="citation">[305]</span>. &quot;Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors&quot; <span class="citation">[306]</span>. &quot;However, it encounters challenges such as poor readability, and language mixing&quot; <span class="citation">[307]</span>. &quot;To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL&quot; <span class="citation">[308]</span>. &quot;DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks&quot; <span class="citation">[309]</span>.</div>

<div class="quote">&quot;DeepSeek-R1 achieves a score of $7 9 . 8 %$ Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217&quot; <span class="citation">[310]</span>. &quot;On MATH-500, it attains an impressive score of $9 7 . 3 %$ , performing on par with OpenAI-o1-1217 and significantly outperforming other models&quot; <span class="citation">[311]</span>. &quot;On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming $9 6 . 3 %$ human participants in the competition&quot; <span class="citation">[312]</span>.</div>

<div class="quote">&quot;On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeekR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of $9 0 . 8 %$ on MMLU, $8 4 . 0 %$ on MMLU-Pro, and $7 1 . 5 %$ on GPQA Diamond&quot; <span class="citation">[313]</span>. &quot;While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks&quot; <span class="citation">[314]</span>.</div>

<h3>7.3 Training Methodology</h3>

<div class="quote">&quot;We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step&quot; <span class="citation">[315]</span>. &quot;This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero&quot; <span class="citation">[316]</span>. &quot;DeepSeekR1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community&quot; <span class="citation">[317]</span>. &quot;Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through ${ \\\\mathrm { R L } } ,$ without the need for SFT&quot; <span class="citation">[318]</span>.</div>

<div class="quote">&quot;Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor&quot; <span class="citation">[319]</span>. &quot;In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL&quot; <span class="citation">[320]</span>.</div>

<div class="quote">&quot;Group Relative Policy Optimization (GRPO) is introduced in DeepSeekMath&quot; <span class="citation">[321]</span>, and used in other DeepSeek works, e.g., DeepSeek-V3 and DeepSeek-R1. &quot;GRPO can be viewed as PPO-inspired algorithm with a very similar surrogate loss, but it avoids learning a value function with another copy of the original policy language model&quot; <span class="citation">[322]</span>. This brings two posited benefits: &quot;Avoiding the challenge of learning a value function from a LM backbone&quot; <span class="citation">[323]</span> and &quot;Saves memory by not needing to keep another set of model weights in memory&quot; <span class="citation">[324]</span>.</div>

<div class="quote">&quot;Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs&quot; <span class="citation">[507]</span>. &quot;It was introduced in the <a href="https://arxiv.org/abs/2402.03300" target="_blank">DeepSeekMath</a> paper in the context of mathematical reasoning&quot; <span class="citation">[508]</span>. &quot;GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model&quot; <span class="citation">[509]</span>. &quot;Instead, it estimates baselines from group scores, reducing memory usage and computational overhead&quot; <span class="citation">[510]</span>. &quot;GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness&quot; <span class="citation">[511]</span>.</div>

<div class="quote">&quot;Starting with DeepSeek V3, they applied GRPO to unsupervised reasoning text completions rule-based reward models that focused on aspects like format, mathematics, and coding&quot; <span class="citation">[512]</span>. &quot;Accuracy rewards: Evaluate whether the response is correct, correct result or compiled LeetCode problem&quot; <span class="citation">[513]</span>. &quot;Format rewards: Evaluate the format that enforces the model to put its thinking process between &#x27;&#x27; and &#x27;&#x27; tags&quot; <span class="citation">[514]</span>. &quot;This leads to a pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, reaching performance levels comparable to OpenAI-o1-0912 alongside output token length per problem increasing, indicating the model naturally learns to solve tasks with more thinking time/token generation&quot; <span class="citation">[515]</span>. &quot;This has the drawback of leading to poor readability and language mixing but it was solved for R1 using a multi-stage approach with alternating SFT → RL steps&quot; <span class="citation">[516]</span>.</div>

<div class="quote">&quot;To prevent the early unstable cold start phase of reinforcement training (RL) training from the base model, the team started with supervised fine-tuning&quot; <span class="citation">[517]</span>. &quot;Collected up to 10k token-long chain-of-thought (CoT) using the fine-tuned models, R1-zero and human annotator&quot; <span class="citation">[518]</span>. &quot;The data is used to fine-tune Deepseek V3 base to improve readbility and coherence&quot; <span class="citation">[519]</span>. &quot;Used the same RL pipeline as R1-Zero, focusing on reasoning-intensive tasks such as coding and math using the same Rule-Based Reward Models&quot; <span class="citation">[520]</span>. &quot;This time, an additional reward for &#x27;language consistency&#x27; is used to help the model stick to the same language&quot; <span class="citation">[521]</span>.</div>

<div class="quote">&quot;Generated large synthetic dataset using Reject Sampling (RS) focusing on writing, role-playing, and other general-purpose tasks&quot; <span class="citation">[522]</span>. &quot;The model from Stage 2 was used with Deepseek V3 as a Judge to generate 600k reasoning-related samples and 200k for writing, role-playing, and other general-purpose tasks using portions of the SFT dataset of DeepSeek-V3 or regenerating them with CoT included&quot; <span class="citation">[523]</span>. &quot;In the Final Stage, GRPO is used again with a combination of Rule-Based and Outcome Reward Models to improve the model&#x27;s helpfulness and harmlessness&quot; <span class="citation">[524]</span>. &quot;Leading to the <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" target="_blank">Deepseek R1</a> model&quot; <span class="citation">[525]</span>.</div>

<div class="quote">&quot;DeepSeek didn&#x27;t use Monte Carlo Tree Search (MCTS) or Process Reward Models (PRM)&quot; <span class="citation">[526]</span>. &quot;Fine-tuning before applying GRPO can actually make the training process faster and more stable&quot; <span class="citation">[527]</span>. &quot;Rule-based rewards focused on accuracy and format are more effective than complex rewards models&quot; <span class="citation">[528]</span>.</div>

<div class="quote">&quot;In order to save the training costs of ${ \\\\scriptstyle \\\\mathrm { R L } } ,$ , we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead&quot; <span class="citation">[325]</span>. &quot;Specifically, for each question $q ,$ , GRPO samples a group of outputs ${ o \\_ { 1 } , o \\_ { 2 } , \\\\cdots , o \\_ { G } }$ from the old policy $\\\\pi \\_ { \\\\theta \\_ { o l d } }$ and then optimizes the policy model $\\\\scriptstyle { \\\\pi \\_ { \\\\theta } }$ by maximizing the following objective&quot; <span class="citation">[326]</span>. &quot;The reward is the source of the training signal, which decides the optimization direction of RL&quot; <span class="citation">[327]</span>. &quot;To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: Accuracy rewards: The accuracy reward model evaluates whether the response is correct&quot; <span class="citation">[328]</span>. &quot;For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness&quot; <span class="citation">[329]</span>. &quot;Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases&quot; <span class="citation">[330]</span>. &quot;Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between &#x27;&#x27; and $&#x27; \\_ { &lt; }$ /think&gt;&#x27; tags&quot; <span class="citation">[331]</span>.</div>

<div class="quote">&quot;We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline&quot; <span class="citation">[332]</span>. &quot;To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions&quot; <span class="citation">[333]</span>. &quot;This template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer&quot; <span class="citation">[334]</span>. &quot;We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strategies—to ensure that we can accurately observe the model&#x27;s natural progression during the RL process&quot; <span class="citation">[335]</span>.</div>

<div class="quote">&quot;Figure 2 depicts the performance trajectory of DeepSeekR1-Zero on the AIME 2024 benchmark throughout the RL training process&quot; <span class="citation">[336]</span>. &quot;As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances&quot; <span class="citation">[337]</span>. &quot;Notably, the average pass $@ 1$ score on AIME 2024 shows a significant increase, jumping from an initial $1 5 . 6 %$ to an impressive $7 1 . 0 %$ , reaching performance levels comparable to OpenAI-o1-0912&quot; <span class="citation">[338]</span>. &quot;This significant improvement highlights the efficacy of our RL algorithm in optimizing the model&#x27;s performance over time&quot; <span class="citation">[339]</span>. &quot;The findings reveal that RL empowers DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data&quot; <span class="citation">[340]</span>. &quot;This is a noteworthy achievement, as it underscores the model&#x27;s ability to learn and generalize effectively through RL alone&quot; <span class="citation">[341]</span>. &quot;Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting&quot; <span class="citation">[342]</span>. &quot;For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero&#x27;s performance escalates from $7 1 . 0 %$ to $8 6 . 7 %$ , thereby exceeding the performance of OpenAI-o1-0912&quot; <span class="citation">[343]</span>. &quot;The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks&quot; <span class="citation">[344]</span>.</div>

<div class="quote">&quot;The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously&quot; <span class="citation">[345]</span>. &quot;By initiating RL directly from the base model, we can closely monitor the model&#x27;s progression without the influence of the supervised fine-tuning stage&quot; <span class="citation">[346]</span>. &quot;This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks&quot; <span class="citation">[347]</span>. &quot;As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement throughout the training process&quot; <span class="citation">[348]</span>. &quot;This improvement is not the result of external adjustments but rather an intrinsic development within the model&quot; <span class="citation">[349]</span>. &quot;DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation&quot; <span class="citation">[350]</span>. &quot;This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth&quot; <span class="citation">[351]</span>.</div>

<div class="quote">&quot;One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases&quot; <span class="citation">[352]</span>. &quot;Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously&quot; <span class="citation">[353]</span>. &quot;These behaviors are not explicitly programmed but instead emerge as a result of the model&#x27;s interaction with the reinforcement learning environment&quot; <span class="citation">[354]</span>. &quot;This spontaneous development significantly enhances DeepSeek-R1-Zero&#x27;s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy&quot; <span class="citation">[355]</span>. &quot;A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an &#x27;aha moment&#x27;&quot; <span class="citation">[356]</span>. &quot;This moment, as illustrated in Table 3, occurs in an intermediate version of the model&quot; <span class="citation">[357]</span>. &quot;During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach&quot; <span class="citation">[358]</span>. &quot;This behavior is not only a testament to the model&#x27;s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes&quot; <span class="citation">[359]</span>. &quot;This moment is not only an &#x27;aha moment&#x27; for the model but also for the researchers observing its behavior&quot; <span class="citation">[360]</span>. &quot;It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies&quot; <span class="citation">[361]</span>. &quot;The &#x27;aha moment&#x27; serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future&quot; <span class="citation">[362]</span>.</div>

<div class="quote">&quot;Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues&quot; <span class="citation">[363]</span>. &quot;For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing&quot; <span class="citation">[364]</span>. &quot;To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data&quot; <span class="citation">[365]</span>. &quot;Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?&quot; <span class="citation">[366]</span>. &quot;To address these questions, we design a pipeline to train DeepSeek-R1&quot; <span class="citation">[367]</span>. &quot;The pipeline consists of four stages, outlined as follows&quot; <span class="citation">[368]</span>.</div>

<div class="quote">&quot;To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators&quot; <span class="citation">[369]</span>. &quot;Compared to DeepSeek-R1-Zero, the advantages of cold start data include: Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading&quot; <span class="citation">[370]</span>. &quot;Responses may mix multiple languages or lack markdown formatting to highlight answers for users&quot; <span class="citation">[371]</span>. &quot;In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly&quot; <span class="citation">[372]</span>. &quot;Here, we define the output format as \\|special\\_token\\|\\|special\\_token\\|, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results&quot; <span class="citation">[373]</span>. &quot;Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero&quot; <span class="citation">[374]</span>. &quot;We believe the iterative training is a better way for reasoning models&quot; <span class="citation">[375]</span>.</div>

<div class="quote">&quot;After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero&quot; <span class="citation">[376]</span>. &quot;This phase focuses on enhancing the model&#x27;s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions&quot; <span class="citation">[377]</span>. &quot;During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages&quot; <span class="citation">[378]</span>. &quot;To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT&quot; <span class="citation">[379]</span>. &quot;Although ablation experiments show that such alignment results in a slight degradation in the model&#x27;s performance, this reward aligns with human preferences, making it more readable&quot; <span class="citation">[380]</span>. &quot;Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward&quot; <span class="citation">[381]</span>. &quot;We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks&quot; <span class="citation">[382]</span>.</div>

<div class="quote">&quot;When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round&quot; <span class="citation">[383]</span>. &quot;Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model&#x27;s capabilities in writing, role-playing, and other general-purpose tasks&quot; <span class="citation">[384]</span>. &quot;Specifically, we generate the data and fine-tune the model as described below&quot; <span class="citation">[385]</span>. &quot;Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training&quot; <span class="citation">[386]</span>. &quot;In the previous stage, we only included data that could be evaluated using rule-based rewards&quot; <span class="citation">[387]</span>. &quot;However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment&quot; <span class="citation">[388]</span>. &quot;Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks&quot; <span class="citation">[389]</span>. &quot;For each prompt, we sample multiple responses and retain only the correct ones&quot; <span class="citation">[390]</span>. &quot;In total, we collect about 600k reasoning related training samples&quot; <span class="citation">[391]</span>. &quot;Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3&quot; <span class="citation">[392]</span>. &quot;For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting&quot; <span class="citation">[393]</span>. &quot;However, for simpler queries, such as &#x27;hello&#x27; we do not provide a CoT in response&quot; <span class="citation">[394]</span>. &quot;In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning&quot; <span class="citation">[395]</span>. &quot;We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples&quot; <span class="citation">[396]</span>.</div>

<div class="quote">&quot;To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model&#x27;s helpfulness and harmlessness while simultaneously refining its reasoning capabilities&quot; <span class="citation">[397]</span>. &quot;Specifically, we train the model using a combination of reward signals and diverse prompt distributions&quot; <span class="citation">[398]</span>. &quot;For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains&quot; <span class="citation">[399]</span>. &quot;For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios&quot; <span class="citation">[400]</span>. &quot;We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts&quot; <span class="citation">[401]</span>. &quot;For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process&quot; <span class="citation">[402]</span>. &quot;For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process&quot; <span class="citation">[403]</span>. &quot;Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness&quot; <span class="citation">[404]</span>.</div>

<hr>

<h2>8. Training Methods and Technical Details</h2>

<h3>8.1 Data Collection and Preprocessing</h3>

<div class="quote">&quot;Like previous GPT models, the GPT‑4 base model was trained to predict the next word in a document, and was trained using publicly available data (such as internet data) as well as data we&#x27;ve licensed&quot; <span class="citation">[47]</span>. The data &quot;is a web-scale corpus of data including correct and incorrect solutions to math problems, weak and strong reasoning, self-contradictory and consistent statements, and representing a great variety of ideologies and ideas&quot; <span class="citation">[48]</span>.</div>

<h3>8.2 Compute Infrastructure</h3>

<div class="quote">&quot;Over the past two years, we rebuilt our entire deep learning stack and, together with Azure, co-designed a supercomputer from the ground up for our workload&quot; <span class="citation">[49]</span>. &quot;A year ago, we trained GPT‑3.5 as a first &#x27;test run&#x27; of the system&quot; <span class="citation">[50]</span>. &quot;We found and fixed some bugs and improved our theoretical foundations&quot; <span class="citation">[51]</span>. &quot;As a result, our GPT‑4 training run was (for us at least!) unprecedentedly stable, becoming our first large model whose training performance we were able to accurately predict ahead of time&quot; <span class="citation">[52]</span>.</div>

<h3>8.3 Optimization and Scaling</h3>

<div class="quote">&quot;We developed infrastructure and optimization that have very predictable behavior across multiple scales&quot; <span class="citation">[53]</span>. &quot;To verify this scalability, we accurately predicted in advance GPT‑4&#x27;s final loss on our internal codebase (not part of the training set) by extrapolating from models trained using the same methodology but using 10,000x less compute&quot; <span class="citation">[54]</span>.</div>

<h3>8.4 Reinforcement Learning from Human Feedback (RLHF)</h3>

<div class="quote">&quot;To align it with the user&#x27;s intent within guardrails, we fine-tune the model&#x27;s behavior using reinforcement learning with human feedback ( <a href="https://openai.com/index/learning-from-human-preferences/" target="_blank">RLHF⁠</a>)&quot; <span class="citation">[55]</span>. &quot;Note that the model&#x27;s capabilities seem to come primarily from the pre-training process—RLHF does not improve exam performance (without active effort, it actually degrades it)&quot; <span class="citation">[56]</span>. &quot;But steering of the model comes from the post-training process—the base model requires prompt engineering to even know that it should answer the questions&quot; <span class="citation">[57]</span>.</div>


<div class="quote">&quot;Reinforcement learning from Human Feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems&quot; <span class="citation">[405]</span>. &quot;The basic pipeline for RLHF involves three steps. First, a language model that can follow user questions must be trained&quot; <span class="citation">[406]</span>. &quot;Second, human preference data must be collected for the training of a reward model of human preferences&quot; <span class="citation">[407]</span>. &quot;Finally, the language model can be optimized with an RL optimizer of choice, by sampling generations and rating them with respect to the reward model&quot; <span class="citation">[408]</span>. &quot;RLHF has been applied to many domains successfully, with complexity increasing as the techniques have matured&quot; <span class="citation">[409]</span>. &quot;In modern language model training, RLHF is one component of post-training&quot; <span class="citation">[410]</span>. &quot;Post-training is a more complete set of techniques and best-practices to make language models more useful for downstream tasks&quot; <span class="citation">[411]</span>. &quot;Post-training can be summarized as using three optimization methods: Instruction / Supervised Finetuning (IFT/SFT), Preference Finetuning (PreFT), and Reinforcement Finetuning (RFT)&quot; <span class="citation">[412]</span>. &quot;Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and form the base of instruction following abilities&quot; <span class="citation">[413]</span>. &quot;Preference Finetuning (PreFT), where we align to human preferences (and get smaller bump in capabilities at the same time)&quot; <span class="citation">[414]</span>. &quot;Reinforcement Finetuning (RFT). The newest type of post-training that boosts performance on verifiable domains&quot; <span class="citation">[415]</span>. &quot;This book focuses on the second area, preference finetuning, which has more complexity than instruction tuning and is far more established than Reinforcement Finetuning&quot; <span class="citation">[416]</span>. &quot;RLHF colloquially is what led to modern post-training&quot; <span class="citation">[417]</span>. &quot;The core role of this book, beyond teaching the techniques for doing RLHF, is to distill intuition as to why RLHF is crucial to modern AI models&quot; <span class="citation">[418]</span>. &quot;Modern research has established RLHF as a general method to integrate subtle stylistic and related behavioral features into the models&quot; <span class="citation">[419]</span>. &quot;Compared to other techniques for post-training, such as instruction finetuning, RLHF generalizes far better across domains&quot; <span class="citation">[420]</span>. &quot;Instruction finetuning is training the model to predict the next certain token when the text preceding is close to examples it has seen&quot; <span class="citation">[421]</span>. &quot;RLHF on the other hand tunes the responses on the response level rather than looking at the next token specifically&quot; <span class="citation">[422]</span>. &quot;RLHF also shows a model which type of response it should avoid, i.e. negative feedback&quot; <span class="citation">[423]</span>. &quot;The training to achieve this is often called a contrastive loss function and is referenced throughout this book&quot; <span class="citation">[424]</span>. &quot;While this flexibility is a major advantage of RLHF, it comes with implementation challenges&quot; <span class="citation">[425]</span>. &quot;Largely, these center on how to control the optimization&quot; <span class="citation">[426]</span>. &quot;Implementing RLHF often requires training a reward model, of which best practices are not strongly established and depend on the area of application&quot; <span class="citation">[427]</span>. &quot;The optimization itself is prone to over-optimization because our reward signal is at best a proxy objective, requiring regularization&quot; <span class="citation">[428]</span>. &quot;Effective RLHF requires a strong starting point, so RLHF cannot be a solution to every problem alone and needs to be approached in a broader lens of post-training&quot; <span class="citation">[429]</span>. &quot;Due to this complexity, implementing RLHF is far more costly than simple instruction finetuning and can come with unexpected challenges such as length bias&quot; <span class="citation">[430]</span>. &quot;For projects where performance matters, RLHF is established as being crucial to achieving a strong finetuned model, but it is more expensive in compute, data costs, and time&quot; <span class="citation">[431]</span>. &quot;The intuition I&#x27;ve been using to understand the potential of post-training is called the elicitation interpretation of post-training, where all we are doing is extracting and amplifying valuable behaviors in the base model&quot; <span class="citation">[432]</span>. &quot;The best post-training teams extract a ton of performance in a very short time frame&quot; <span class="citation">[433]</span>. &quot;The set of techniques is everything after the end of most of pretraining&quot; <span class="citation">[434]</span>. &quot;This theory folds in with the reality that the majority of gains users are seeing are from post-training because it implies that there is more latent potential in a model pretraining on the internet than we can teach the model simply&quot; <span class="citation">[435]</span>.</div>

<hr>
<h3>8.5 Architecture Modifications</h3>

<div class="quote">&quot;Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture&quot; <span class="citation">[255]</span>. &quot;In MoE models, a single token activates only a fraction of the total parameters&quot; <span class="citation">[256]</span>. &quot;MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model&quot; <span class="citation">[257]</span>.</div>

<hr>

<h2>9. Evaluation and Benchmarking</h2>

<h3>9.1 Academic Benchmarks</h3>

<div class="quote">&quot;GPT‑4 considerably outperforms existing large language models, alongside most state-of-the-art (SOTA) models which may include benchmark-specific crafting or additional training protocols&quot; <span class="citation">[58]</span>. GPT-4 demonstrated strong performance, with &quot;GPT-4 achieves 86.4% on MMLU (5-shot)&quot; <span class="citation">[59]</span>, &quot;GPT-4 achieves 95.3% on HellaSwag (10-shot)&quot; <span class="citation">[60]</span>, &quot;GPT-4 achieves 96.3% on ARC (25-shot)&quot; <span class="citation">[61]</span>, and &quot;GPT-4 achieves 67.0% on HumanEval (0-shot)&quot; <span class="citation">[62]</span>.</div>

<h3>9.2 Competition-Level Evaluations</h3>

<div class="quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot; <span class="citation">[63]</span>. &quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot; <span class="citation">[64]</span>. &quot;Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval)&quot; <span class="citation">[181]</span>. &quot;Gemini 2.5 Pro Thinking&quot; achieves &quot;88.0%&quot; on &quot;AIME 2025&quot; <span class="citation">[232]</span>.</div>

<h3>9.3 Internal Evaluation Benchmarks</h3>

<p>Companies maintain &quot;internal eval benchmarks&quot; <span class="citation">[558]</span> that are not publicly disclosed. &quot;These benchmarks are used to assess model capabilities before public release, though specific details remain proprietary&quot; <span class="citation">[559]</span>.</p>

<hr>

<h2>10. Safety, Alignment, and Red-Teaming</h2>

<h3>10.1 Safety Measures</h3>

<div class="quote">&quot;Our models are subjected to rigorous testing and have been trained to reduce misuse&quot; <span class="citation">[182]</span>. &quot;We engaged over 50 experts from domains such as AI alignment risks, cybersecurity, biorisk, trust and safety, and international security to adversarially test the model&quot; <span class="citation">[65]</span>. &quot;Their findings specifically enabled us to test model behavior in high-risk areas which require expertise to evaluate&quot; <span class="citation">[66]</span>.</div>

<h3>10.2 Red-Teaming</h3>

<div class="quote">&quot;We&#x27;ve engaged with external experts to test and refine the safety mechanisms within this latest model&quot; <span class="citation">[183]</span>. &quot;We recently provided Claude 3.5 Sonnet to the UK&#x27;s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation&quot; <span class="citation">[184]</span>. &quot;The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs <a href="https://www.commerce.gov/news/press-releases/2024/04/us-and-uk-announce-partnership-science-ai-safety" target="_blank">announced earlier this year</a>&quot; <span class="citation">[185]</span>.</div>

<h3>10.3 Alignment Techniques</h3>

<div class="quote">&quot;Claude: Employs Constitutional AI (CAI) with RLAIF, using AI-generated feedback based on explicit principles to guide behavior towards being&quot; <span class="citation">[529]</span>. Various alignment techniques are employed, including &quot;Constitutional AI, Deliberative Alignment, RLAIF&quot; <span class="citation">[560]</span>. &quot;Specific implementation details remain proprietary, though research indicates these techniques are critical for ensuring model safety and alignment with human values&quot; <span class="citation">[561]</span>.</div>

<hr>

<h2>11. Challenges in Researching Frontier AI Models</h2>

<h3>11.1 Information Opacity</h3>

<div class="quote">&quot;Key technical details are intentionally undisclosed or obscured&quot; <span class="citation">[562]</span>, making comprehensive research extremely difficult. &quot;These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot; <span class="citation">[563]</span>.</div>

<h3>11.2 Rapid Evolution</h3>

<p>The field &quot;evolves so fast that even partial information becomes outdated within weeks&quot; <span class="citation">[564]</span>. &quot;This rapid evolution means that research findings may become obsolete quickly, requiring constant updates and verification&quot; <span class="citation">[565]</span>.</p>

<h3>11.3 Verification Challenges</h3>

<div class="quote">&quot;Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware&quot; <span class="citation">[566]</span>. &quot;This makes it difficult to verify claims about model capabilities, training methods, and performance&quot; <span class="citation">[567]</span>.</div>

<h3>11.4 Speculation and Misinformation</h3>

<div class="quote">&quot;Widespread speculation, leaks, and misinformation drown out credible analysis&quot; <span class="citation">[568]</span>. &quot;This creates challenges in distinguishing accurate information from speculation or misinformation&quot; <span class="citation">[569]</span>.</div>

<hr>

<h2>12. Future Directions and Open Questions</h2>

<h3>12.1 Scaling Laws</h3>

<p>Research continues into scaling laws and their implications for future model development. &quot;The relationship between compute, model size, dataset size, and performance&quot; <span class="citation">[570]</span> remains an active area of research, though specific findings are often proprietary.</p>

<h3>12.2 Synthetic Data and Distillation</h3>

<div class="quote">&quot;The use of larger models to generate data for training smaller models&quot; <span class="citation">[571]</span> represents an emerging area of research. &quot;This approach could potentially reduce training costs while maintaining performance&quot; <span class="citation">[572]</span>.</div>

<h3>12.3 Architecture Innovations</h3>

<p>Ongoing research into &quot;attention mechanisms&quot; <span class="citation">[573]</span>, including &quot;FlashAttention, Slim Attention, Scalable Softmax, Hyena, Mamba, RWKV, RetNet, and Differential Transformers&quot; <span class="citation">[574]</span>, continues to drive improvements in model efficiency and performance.</p>

<h3>12.4 Parallelism Strategies</h3>

<p>Research into &quot;data parallelism, model parallelism, tensor parallelism, pipeline parallelism, and hybrid parallelism&quot; <span class="citation">[575]</span> continues to optimize training efficiency for large-scale models.</p>

<hr>

<h2>13. Conclusion</h2>

<div class="quote">&quot;Researching frontier AI model capabilities and training methods presents significant challenges due to intentional opacity, rapid evolution, and verification difficulties&quot; <span class="citation">[576]</span>. &quot;While some information is available through official announcements, research papers, and technical reports, many critical details remain proprietary&quot; <span class="citation">[577]</span>. &quot;The field continues to evolve rapidly, with new models and techniques emerging regularly&quot; <span class="citation">[578]</span>. &quot;Understanding these models requires careful analysis of available information while acknowledging the limitations imposed by competitive secrecy and proprietary development practices&quot; <span class="citation">[579]</span>.</div>

<hr>










<div class="references">
<h2>References</h2>

<div class="reference-item">
<span class="reference-number">[1]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We are introducing GPT‑5, our best AI system yet&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[2]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[3]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[4]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPT‑5 pro, a version with extended reasoning for even more comprehensive and accurate answers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[5]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 not only outperforms previous models on benchmarks and answers questions more quickly, but—most importantly—is more useful for real-world queries&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[6]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We&#x27;ve made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, while leveling up GPT‑5&#x27;s performance in three of ChatGPT&#x27;s most common uses: writing, coding, and health&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[7]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is our strongest coding model to date&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[8]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It shows particular improvements in <strong>complex front‑end generation</strong> and <strong>debugging larger repositories</strong>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[9]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It can often create beautiful and responsive websites, apps, and games with an eye for aesthetic sensibility in just one prompt, intuitively and tastefully turning ideas into reality&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[10]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Early testers also noted its design choices, with a much better understanding of things like spacing, typography, and white space&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[11]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is our most capable writing collaborator yet, able to help you steer and translate rough ideas into <strong>compelling, resonant writing</strong> with literary depth and rhythm&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[12]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It more reliably handles writing that involves structural ambiguity, such as sustaining unrhymed iambic pentameter or free verse that flows naturally, combining respect for form with expressive clarity&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[13]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is our best model yet for health-related questions, empowering users to be informed about and advocate for their health&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[14]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;The model scores significantly higher than any previous model on <a href="https://openai.com/index/healthbench/" target="_blank"><strong>HealthBench</strong> ⁠</a>, an evaluation we published earlier this year based on realistic scenarios and physician-defined criteria&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[15]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Compared to previous models, it acts more like an active thought partner, proactively flagging potential concerns and asking questions to give more helpful answers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[16]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is much smarter across the board, as reflected by its performance on academic and human-evaluated benchmarks, particularly in math, coding, visual perception, and health&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[17]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It sets a new <strong>state of the art across math (94.6% on AIME 2025 without tools), real-world coding (74.9% on SWE-bench Verified, 88% on Aider Polyglot), multimodal understanding (84.2% on MMMU), and health (46.2% on HealthBench Hard)</strong>—and those gains show up in everyday use&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[18]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;With GPT‑5 pro&#x27;s extended reasoning, the model also sets a new SOTA on <strong>GPQA</strong>, scoring 88.4% without tools&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[19]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 was trained on Microsoft Azure AI supercomputers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[20]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 advances the frontier on safety&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[21]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In the past, ChatGPT relied primarily on refusal-based safety training: based on the user&#x27;s prompt, the model should either comply or refuse&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[22]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;While this type of training works well for explicitly malicious prompts, it can struggle to handle situations where the user&#x27;s intent is unclear, or information could be used in benign or malicious ways&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[23]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;For GPT‑5, we introduced a new form of safety-training — safe completions — which teaches the model to give the most helpful answer where possible while still staying within safety boundaries&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[24]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is significantly less likely to hallucinate than our previous models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[25]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;With web search enabled on anonymized prompts representative of ChatGPT production traffic, GPT‑5&#x27;s responses are ~45% less likely to contain a factual error than GPT‑4o, and when thinking, GPT‑5&#x27;s responses are ~80% less likely to contain a factual error than OpenAI o3&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[26]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 (with thinking) more honestly communicates its actions and capabilities to the user—especially for tasks which are impossible, underspecified, or missing key tools&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[27]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In order to achieve a high reward during training, reasoning models may learn to lie about successfully completing a task or be overly confident about an uncertain answer&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[28]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We evaluated deception rates on settings involving impossible coding tasks and missing multimodal assets, and found that GPT‑5 (with thinking) is less deceptive than o3 across the board&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[29]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;On a large set of conversations representative of real production ChatGPT traffic, we&#x27;ve reduced rates of deception from 4.8% for o3 to 2.1% of GPT‑5 reasoning responses&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[30]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;OpenAI o1 ranks in the 89th percentile on competitive programming questions (Codeforces), places among the top 500 students in the US in a qualifier for the USA Math Olympiad (AIME), and exceeds human PhD-level accuracy on a benchmark of physics, biology, and chemistry problems (GPQA)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[31]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[32]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[33]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;A score of 13.9 places it among the top 500 students nationally and above the cutoff for the USA Mathematical Olympiad&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[34]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We also evaluated o1 on GPQA diamond, a difficult intelligence benchmark which tests for expertise in chemistry, physics and biology&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[35]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;In order to compare models to humans, we recruited experts with PhDs to answer GPQA-diamond questions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[36]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We found that o1 surpassed the performance of those human experts, becoming the first model to do so on this benchmark&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[37]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;These results do not imply that o1 is more capable than a PhD in all respects — only that the model is more proficient in solving some problems that a PhD would be expected to solve&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[38]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[39]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We have found that the performance of o1 consistently improves with more reinforcement learning (train-time compute) and with more time spent thinking (test-time compute)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[40]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;The constraints on scaling this approach differ substantially from those of LLM pretraining, and we are continuing to investigate them&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[41]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Similar to how a human may think for a long time before responding to a difficult question, o1 uses a chain of thought when attempting to solve a problem&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[42]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Through reinforcement learning, o1 learns to hone its chain of thought and refine the strategies it uses&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[43]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;It learns to recognize and correct its mistakes&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[44]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;It learns to break down tricky steps into simpler ones&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[45]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;It learns to try a different approach when the current one isn&#x27;t working&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[46]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;This process dramatically improves the model&#x27;s ability to reason&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[47]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;Like previous GPT models, the GPT‑4 base model was trained to predict the next word in a document, and was trained using publicly available data (such as internet data) as well as data we&#x27;ve licensed&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[48]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;is a web-scale corpus of data including correct and incorrect solutions to math problems, weak and strong reasoning, self-contradictory and consistent statements, and representing a great variety of ideologies and ideas&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[49]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;Over the past two years, we rebuilt our entire deep learning stack and, together with Azure, co-designed a supercomputer from the ground up for our workload&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[50]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;A year ago, we trained GPT‑3.5 as a first &#x27;test run&#x27; of the system&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[51]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;We found and fixed some bugs and improved our theoretical foundations&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[52]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;As a result, our GPT‑4 training run was (for us at least!) unprecedentedly stable, becoming our first large model whose training performance we were able to accurately predict ahead of time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[53]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;We developed infrastructure and optimization that have very predictable behavior across multiple scales&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[54]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;To verify this scalability, we accurately predicted in advance GPT‑4&#x27;s final loss on our internal codebase (not part of the training set) by extrapolating from models trained using the same methodology but using 10,000x less compute&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[55]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;To align it with the user&#x27;s intent within guardrails, we fine-tune the model&#x27;s behavior using reinforcement learning with human feedback ( <a href="https://openai.com/index/learning-from-human-preferences/" target="_blank">RLHF⁠</a>)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[56]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;Note that the model&#x27;s capabilities seem to come primarily from the pre-training process—RLHF does not improve exam performance (without active effort, it actually degrades it)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[57]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;But steering of the model comes from the post-training process—the base model requires prompt engineering to even know that it should answer the questions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[58]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;GPT‑4 considerably outperforms existing large language models, alongside most state-of-the-art (SOTA) models which may include benchmark-specific crafting or additional training protocols&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[59]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;GPT-4 achieves 86.4% on MMLU (5-shot)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[60]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;GPT-4 achieves 95.3% on HellaSwag (10-shot)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[61]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;GPT-4 achieves 96.3% on ARC (25-shot)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[62]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;GPT-4 achieves 67.0% on HumanEval (0-shot)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[63]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[64]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[65]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;We engaged over 50 experts from domains such as AI alignment risks, cybersecurity, biorisk, trust and safety, and international security to adversarially test the model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[66]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;GPT-4.&quot; OpenAI Research, March 14, 2023. https://openai.com/research/gpt-4</div>
<div class="reference-quote">&quot;Their findings specifically enabled us to test model behavior in high-risk areas which require expertise to evaluate&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[67]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;On the 2024 AIME exams, GPT‑4o only solved on average 12% (1.8/15) of problems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[68]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[69]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is a unified system with a <strong>smart, efficient model</strong> that answers most questions, a <strong>deeper reasoning model</strong> (GPT‑5 thinking) for harder problems, and a <strong>real‑time router</strong> that quickly decides which to use based on conversation type, complexity, tool needs, and your explicit intent (for example, if you say &#x27;think hard about this&#x27; in the prompt)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[70]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[71]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Once usage limits are reached, a mini version of each model handles remaining queries&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[72]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In the near future, we plan to integrate these capabilities into a single model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[73]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 gets more value out of less thinking time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[74]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In our evaluations, GPT‑5 (with thinking) performs better than OpenAI o3 with 50-80% less output tokens across capabilities, including visual reasoning, agentic coding, and graduate-level scientific problem solving&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[75]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Overall, GPT‑5 is <strong>less effusively agreeable</strong>, uses <strong>fewer unnecessary emojis</strong>, and is more subtle and thoughtful in follow‑ups compared to GPT‑4o&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[76]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;It should feel less like &#x27;talking to AI&#x27; and more like <strong>chatting with a helpful friend</strong> with PhD‑level intelligence&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[77]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Earlier this year, we <a href="https://openai.com/index/sycophancy-in-gpt-4o/" target="_blank">released an update to GPT‑4o⁠</a> that unintentionally made the model overly sycophantic, or excessively flattering or agreeable&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[78]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We quickly <a href="https://openai.com/index/expanding-on-sycophancy/" target="_blank">rolled back the change⁠</a> and have since worked to understand and reduce this behavior by: Developing new evaluations to measure sycophancy levels, Improving our training so the model is less sycophantic—for instance, adding examples that would normally lead to over-agreement, and then teaching it not to do that&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[79]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In targeted sycophancy evaluations using prompts specifically designed to elicit sycophantic responses, GPT‑5 meaningfully reduced sycophantic replies (from 14.5% to less than 6%)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[80]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;At times, reducing sycophancy can come with reductions in user satisfaction, but the improvements we made cut sycophancy by more than half while also delivering other measurable gains, so users continue to have high-quality, constructive conversations—in line with our goal to <a href="https://openai.com/index/how-we&#x27;re-optimizing-chatgpt/" target="_blank">help people use ChatGPT well⁠</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[81]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is significantly better at instruction following, and we see a corresponding improvement in its ability to follow custom instructions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[82]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We&#x27;re also launching a research preview of four new preset personalities for all ChatGPT users, made possible by the improvements on steerability&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[83]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;These personalities, available initially for text chat and coming later to Voice, let you set how ChatGPT interacts—whether concise and professional, thoughtful and supportive, or a bit sarcastic—without writing custom prompts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[84]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;The four initial options, Cynic, Robot, Listener, and Nerd, are opt-in, adjustable anytime in settings, and designed to match your communication style&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[85]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;All of these new personalities meet or exceed our bar on internal evals for reducing sycophancy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[86]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We look forward to learning and iterating based on early feedback&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[87]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We decided to treat the &#x27;GPT‑5 thinking&#x27; model as High capability in the Biological and Chemical domain, and have implemented strong safeguards to sufficiently minimize the associated risks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[88]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;We rigorously tested the model with our safety evaluations under our <a href="https://openai.com/index/updating-our-preparedness-framework/" target="_blank">Preparedness Framework⁠⁠</a>, completing 5,000 hours of red-teaming with partners like the CAISI and UK AISI&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[89]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Similar to our approach for ChatGPT Agent, while we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm–our <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf" target="_blank">defined threshold⁠(opens in a new window)</a> for High capability–we are taking a precautionary approach and are activating the required safeguards now in order to increase readiness for when such capabilities are available&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[90]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;As a result, &#x27;GPT‑5 thinking&#x27; has a robust safety stack with a multilayered defense system for biology: comprehensive threat modeling, training the model to not output harmful content through our new safe completions paradigm, always-on classifiers and reasoning monitors, and clear enforcement pipelines&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[91]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Read more about our robust safety approach for GPT‑5 in our <a href="https://openai.com/index/gpt-5-system-card/" target="_blank">system card</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[92]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;For the most challenging, complex tasks, we are also releasing GPT‑5 pro, replacing OpenAI o3‑pro, a variant of GPT‑5 that thinks for ever longer, using scaled but efficient parallel test-time compute, to provide the highest quality and most comprehensive answers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[93]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 pro achieves the highest performance in the GPT‑5 family on several challenging intelligence benchmarks, including state-of-the-art performance on GPQA, which contains extremely difficult science questions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[94]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;In evaluations on over 1000 economically valuable, real-world reasoning prompts, external experts preferred GPT‑5 pro over &#x27;GPT‑5 thinking&#x27; 67.8% of the time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[95]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 pro made 22% fewer major errors and excelled in health, science, mathematics, and coding&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[96]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Experts rated its responses as relevant, useful, and comprehensive&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[97]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is the new default in ChatGPT, replacing GPT‑4o, OpenAI o3, OpenAI o4-mini, GPT‑4.1, and GPT‑4.5 for signed-in users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[98]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Just open ChatGPT and type your question; GPT‑5 handles the rest <strong>,</strong> applying reasoning automatically when the response would benefit from it&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[99]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Paid users can still select <strong>&#x27;GPT‑5 Thinking&#x27;</strong> from the model picker, or type something like &#x27;think hard about this&#x27; in the prompt to ensure reasoning is used when generating a response&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[100]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;GPT‑5 is starting to roll out today <strong>to all Plus, Pro, Team, and Free users, with access for Enterprise and Edu coming next week</strong>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[101]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Pro, Plus, and Team users can also start coding with GPT‑5 in the <a href="https://github.com/openai/codex" target="_blank">Codex CLI⁠(opens in a new window)</a> by signing in with ChatGPT&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[102]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;As with GPT‑4o, the difference between free and paid access to GPT‑5 is usage volume&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[103]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Pro subscribers get unlimited access to GPT‑5, and access to <strong>GPT‑5 Pro</strong>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[104]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Plus users can use it comfortably as their default model for everyday questions, with significantly higher usage than free users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[105]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Team, Enterprise, and Edu customers can also use GPT‑5 comfortably as their default model for everyday work, with generous limits that make it easy for entire organizations to rely on GPT‑5&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[106]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;For ChatGPT free-tier users, full reasoning capabilities may take a few days to fully roll out&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[107]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Introducing GPT-5.&quot; OpenAI, August 7, 2025. https://openai.com/index/introducing-gpt-5/</div>
<div class="reference-quote">&quot;Once free users reach their GPT‑5 usage limits, they will transition to <strong>GPT‑5 mini</strong>, a smaller, faster, and highly capable model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[108]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 performance smoothly improves with both train-time and test-time compute&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[109]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;To highlight the reasoning improvement over GPT‑4o, we tested our models on a diverse set of human exams and ML benchmarks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[110]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We show that o1 significantly outperforms GPT‑4o on the vast majority of these reasoning-heavy tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[111]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Unless otherwise specified, we evaluated o1 on the maximal test-time compute setting&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[112]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 greatly improves over GPT-4o on challenging reasoning benchmarks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[113]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Solid bars show pass@1 accuracy and the shaded region shows the performance of majority vote (consensus) with 64 samples&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[114]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;In many reasoning-heavy benchmarks, o1 rivals the performance of human experts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[115]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Recent frontier models do so well on MATH and GSM8K that these benchmarks are no longer effective at differentiating models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[116]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[117]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;On several other ML benchmarks, o1 improved over the state-of-the-art&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[118]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;With its vision perception capabilities enabled, o1 scored 78.2% on MMMU, making it the first model to be competitive with human experts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[119]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;It also outperformed GPT‑4o on 54 out of 57 MMLU subcategories&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[120]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;In addition to exams and academic benchmarks, we also evaluated human preference of o1‑preview vs GPT‑4o on challenging, open-ended prompts in a broad spectrum of domains&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[121]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;In this evaluation, human trainers were shown anonymized responses to a prompt from o1‑preview and GPT‑4o, and voted for which response they preferred&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[122]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1‑preview is preferred to gpt-4o by a large margin in reasoning-heavy categories like data analysis, coding, and math&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[123]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;However, o1‑preview is not preferred on some natural language tasks, suggesting that it is not well-suited for all use cases&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[124]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Chain of thought reasoning provides new opportunities for alignment and safety&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[125]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We found that integrating our policies for model behavior into the chain of thought of a reasoning model is an effective way to robustly teach human values and principles&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[126]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;By teaching the model our safety rules and how to reason about them in context, we found evidence of reasoning capability directly benefiting model robustness: o1‑preview achieved substantially improved performance on key jailbreak evaluations and our hardest internal benchmarks for evaluating our model&#x27;s safety refusal boundaries&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[127]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We believe that using a chain of thought offers significant advances for safety and alignment because (1) it enables us to observe the model thinking in a legible way, and (2) the model reasoning about safety rules is more robust to out-of-distribution scenarios&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[128]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;To stress-test our improvements, we conducted a suite of safety tests and red-teaming before deployment, in accordance with our <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf" target="_blank">Preparedness Framework⁠(opens in a new window)</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[129]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We found that chain of thought reasoning contributed to capability improvements across our evaluations&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[130]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Of particular note, we observed <a href="https://cdn.openai.com/o1-system-card.pdf#page=16" target="_blank">interesting instances of reward hacking⁠(opens in a new window)</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[131]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Detailed results from these evaluations can be found in the accompanying <a href="https://openai.com/index/openai-o1-system-card/" target="_blank">System Card</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[132]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We believe that a hidden chain of thought presents a unique opportunity for monitoring models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[133]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Assuming it is faithful and legible, the hidden chain of thought allows us to &#x27;read the mind&#x27; of the model and understand its thought process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[134]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;For example, in the future we may wish to monitor the chain of thought for signs of manipulating the user&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[135]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;However, for this to work the model must have freedom to express its thoughts in unaltered form, so we cannot train any policy compliance or user preferences onto the chain of thought&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[136]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We also do not want to make an unaligned chain of thought directly visible to users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[137]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Therefore, after weighing multiple factors including user experience, competitive advantage, and the option to pursue the chain of thought monitoring, we have decided not to show the raw chains of thought to users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[138]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We acknowledge this decision has disadvantages&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[139]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We strive to partially make up for it by teaching the model to reproduce any useful ideas from the chain of thought in the answer&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[140]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;For the o1 model series we show a model-generated summary of the chain of thought&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[141]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We trained a model that scored 213 points and ranked in the 49th percentile in the 2024 International Olympiad in Informatics (IOI), by initializing from o1 and training to further improve programming skills&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[142]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;This model competed in the 2024 IOI under the same conditions as the human contestants&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[143]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;It had ten hours to solve six challenging algorithmic problems and was allowed 50 submissions per problem&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[144]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;For each problem, our system sampled many candidate submissions and submitted 50 of them based on a test-time selection strategy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[145]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Submissions were selected based on performance on the IOI public test cases, model-generated test cases, and a learned scoring function&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[146]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;If we had instead submitted at random, we would have only scored 156 points on average, suggesting that this strategy was worth nearly 60 points under competition constraints&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[147]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;With a relaxed submission constraint, we found that model performance improved significantly&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[148]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;When allowed 10,000 submissions per problem, the model achieved a score of 362.14 – above the gold medal threshold – even without any test-time selection strategy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[149]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Finally, we simulated competitive programming contests hosted by Codeforces to demonstrate this model&#x27;s coding skill&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[150]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Our evaluations closely matched competition rules and allowed for 10 submissions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[151]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;GPT‑4o achieved an Elo rating of 808, which is in the 11th percentile of human competitors&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[152]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;This model far exceeded both GPT‑4o and o1—it achieved an Elo rating of 1807, performing better than 93% of competitors&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[153]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;Further fine-tuning on programming competitions improves o1&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[154]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;The improved model ranked in the 49th percentile in the 2024 International Olympiad in Informatics under competition rules&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[155]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;o1 significantly advances the state-of-the-art in AI reasoning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[156]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We plan to release improved versions of this model as we continue iterating&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[157]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We expect these new reasoning capabilities will improve our ability to align models to human values and principles&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[158]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We believe o1 – and its successors – will unlock many new use cases for AI in science, coding, math, and related fields&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[159]</span>
<div class="reference-content">
<div class="reference-citation">OpenAI. &quot;Learning to reason with LLMs.&quot; OpenAI, September 12, 2024. https://openai.com/index/learning-to-reason-with-llms/</div>
<div class="reference-quote">&quot;We are excited for users and API developers to discover how it can improve their daily work&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[160]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[161]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[162]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;marked improvement in grasping nuance, humor, and complex instructions, and is exceptional at writing high-quality content with a natural, relatable tone&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[163]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Claude 3.5 Sonnet operates at twice the speed of Claude 3 Opus&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[164]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;ideal for complex tasks such as context-sensitive customer support and orchestrating multi-step workflows&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[165]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;In an <a href="https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf" target="_blank">internal agentic coding evaluation</a>, Claude 3.5 Sonnet solved 64% of problems, outperforming Claude 3 Opus which solved 38%&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[166]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Our evaluation tests the model&#x27;s ability to fix a bug or add functionality to an open source codebase, given a natural language description of the desired improvement&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[167]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;When instructed and <a href="https://www.anthropic.com/news/tool-use-ga" target="_blank">provided with the relevant tools</a>, Claude 3.5 Sonnet can independently write, edit, and execute code with sophisticated reasoning and troubleshooting capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[168]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Today, we&#x27;re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[169]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made <a href="https://youtu.be/t3nnDXa81Hs" target="_blank">visible to the user</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[170]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;API users also have fine-grained control over _how long_ the model can think for&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[171]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;We&#x27;ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[172]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[173]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;This unified approach also creates a more seamless experience for users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[174]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to <a href="https://www.anthropic.com/research/visible-extended-thinking" target="_blank">think longer before answering</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[175]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[176]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking" target="_blank">extended thinking mode</a>, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[177]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Despite Claude 3.5 Sonnet&#x27;s leap in intelligence, our red teaming assessments have concluded that Claude 3.5 Sonnet remains at ASL-2&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[178]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;We&#x27;ve engaged with external experts to test and refine the safety mechanisms within this latest model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[179]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Claude 3.5 Sonnet to the UK&#x27;s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[180]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">unnecessary refusals by 45%</a> compared to its predecessor&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[181]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Claude 3.5 Sonnet sets new industry benchmarks for graduate-level reasoning (GPQA), undergraduate-level knowledge (MMLU), and coding proficiency (HumanEval)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[182]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;Our models are subjected to rigorous testing and have been trained to reduce misuse&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[183]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;We&#x27;ve engaged with external experts to test and refine the safety mechanisms within this latest model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[184]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;We recently provided Claude 3.5 Sonnet to the UK&#x27;s Artificial Intelligence Safety Institute (UK AISI) for pre-deployment safety evaluation&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[185]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.5 Sonnet.&quot; Anthropic News, June 20, 2024. https://www.anthropic.com/news/claude-3-5-sonnet</div>
<div class="reference-quote">&quot;The UK AISI completed tests of 3.5 Sonnet and shared their results with the US AI Safety Institute (US AISI) as part of a Memorandum of Understanding, made possible by the partnership between the US and UK AISIs <a href="https://www.commerce.gov/news/press-releases/2024/04/us-and-uk-announce-partnership-science-ai-safety" target="_blank">announced earlier this year</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[186]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[187]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Along with the model, we&#x27;re also introducing a command line tool for agentic coding, Claude Code&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[188]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[189]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet is now available on all <a href="https://claude.ai/redirect/website.v1.790f2a5b-c1f3-476a-b7c8-3a83a41660c8/new" target="_blank">Claude</a> plans—including Free, Pro, Team, and Enterprise—as well as the <a href="https://docs.claude.com/en/docs/about-claude/models" target="_blank">Claude Developer Platform</a>, <a href="https://aws.amazon.com/bedrock/claude/" target="_blank">Amazon Bedrock</a>, and <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude" target="_blank">Google Cloud&#x27;s Vertex AI</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[190]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Extended thinking mode is available on all surfaces except the free Claude tier&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[191]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[192]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet: Frontier reasoning made practical&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[193]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In developing our reasoning models, we&#x27;ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[194]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;<a href="https://www.anthropic.com/claude/sonnet#customer-stories" target="_blank">Early testing</a> demonstrated Claude&#x27;s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[195]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Cognition found it far better than any other model at planning code changes and handling full-stack updates&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[196]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Vercel highlighted Claude&#x27;s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[197]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In Canva&#x27;s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[198]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models&#x27; ability to solve real-world software issues&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[199]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[200]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[201]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Beyond traditional benchmarks, it even outperformed all previous models in our <a href="https://www.anthropic.com/research/visible-extended-thinking" target="_blank">Pokémon gameplay tests</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[202]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Since June 2024, Sonnet has been the preferred model for developers worldwide&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[203]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Today, we&#x27;re empowering developers further by introducing <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code" target="_blank">Claude Code</a>—our first agentic coding tool—in a limited research preview&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[204]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[205]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[206]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[207]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;In the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude&#x27;s own understanding of its capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[208]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Our goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[209]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;By <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview#install-and-authenticate" target="_blank">joining this preview</a>, you&#x27;ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[210]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;We&#x27;ve also improved the coding experience on Claude.ai&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[211]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[212]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet is our best coding model to date&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[213]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[214]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;We&#x27;ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[215]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;The <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">system card</a> for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[216]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[217]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[218]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Read the full <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card" target="_blank">system card</a> to learn more&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[219]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[220]</span>
<div class="reference-content">
<div class="reference-citation">Anthropic. &quot;Claude 3.7 Sonnet and Claude Code.&quot; Anthropic News, February 24, 2025. https://www.anthropic.com/news/claude-3-7-sonnet</div>
<div class="reference-quote">&quot;With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what <a href="https://darioamodei.com/machines-of-loving-grace" target="_blank">humans can achieve</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[221]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;Gemini 2.5 is our most intelligent AI model, capable of reasoning through its thoughts before responding, resulting in enhanced performance and improved accuracy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[222]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;Best for coding and highly complex tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[223]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;fast performance on everyday tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[224]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;an enhanced reasoning mode that uses cutting edge research techniques in parallel thinking and reinforcement learning to significantly improve Gemini&#x27;s ability to solve complex problems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[225]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;can better help tackle problems that require creativity, strategic planning, and making improvements step-by-step&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[226]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;Humanity&#x27;s Last Exam (no tools)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[227]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;GPQA diamond&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[228]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;AIME 2025&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[229]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;LiveCodeBench (UI: 1/1/2025-5/1/2025)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[230]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;Gemini 2.5 builds on the best of Gemini — with native multimodality and a long context window&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[231]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;text, image and video modalities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[232]</span>
<div class="reference-content">
<div class="reference-citation">Google DeepMind. &quot;Gemini.&quot; Google DeepMind, 2025. https://deepmind.google/models/gemini/</div>
<div class="reference-quote">&quot;AIME 2025&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[233]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We&#x27;re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[234]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[235]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[236]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[237]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on <a href="https://lmarena.ai/leaderboard" target="_blank">LMArena</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[238]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world&#x27;s smartest LLMs&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[239]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[240]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Behemoth is still training, and we&#x27;re excited to share more details about it even while it&#x27;s still in flight&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[241]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[242]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;In MoE models, a single token activates only a fraction of the total parameters&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[243]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[244]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;As an example, Llama 4 Maverick models have 17B active parameters and 400B total parameters&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[245]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We use alternating dense and mixture-of-experts (MoE) layers for inference efficiency&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[246]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;MoE layers use 128 routed experts and a shared expert&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[247]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Each token is sent to the shared expert and also to one of the 128 routed experts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[248]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;As a result, while all parameters are stored in memory, only a subset of the total parameters are activated while serving these models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[249]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This improves inference efficiency by lowering model serving costs and latency—Llama 4 Maverick can be run on a single NVIDIA H100 DGX host for easy deployment, or with distributed inference for maximum efficiency&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[250]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 models are designed with native multimodality, incorporating early fusion to seamlessly integrate text and vision tokens into a unified model backbone&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[251]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Early fusion is a major step forward, since it enables us to jointly pre-train the model with large amounts of unlabeled text, image, and video data&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[252]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We also improved the vision encoder in Llama 4&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[253]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This is based on MetaCLIP but trained separately in conjunction with a frozen Llama model to better adapt the encoder to the LLM&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[254]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[255]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Our new Llama 4 models are our first models that use a mixture of experts (MoE) architecture&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[256]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;In MoE models, a single token activates only a fraction of the total parameters&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[257]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;MoE architectures are more compute efficient for training and inference and, given a fixed training FLOPs budget, delivers higher quality compared to a dense model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[258]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We developed a new training technique which we refer to as MetaP that allows us to reliably set critical model hyper-parameters such as per-layer learning rates and initialization scales&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[259]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We found that chosen hyper-parameters transfer well across different values of batch size, model width, depth, and training tokens&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[260]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 enables open source fine-tuning efforts by pre-training on 200 languages, including over 100 with over 1 billion tokens each, and overall 10x more multilingual tokens than Llama 3&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[261]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Additionally, we focus on efficient model training by using FP8 precision, without sacrificing quality and ensuring high model FLOPs utilization—while pre-training our Llama 4 Behemoth model using FP8 and 32K GPUs, we achieved 390 TFLOPs/GPU&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[262]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We continued training the model in what we call &#x27;mid-training&#x27; to improve core capabilities with new training recipes including long context extension using specialized datasets&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[263]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This enabled us to enhance model quality while also unlocking best-in-class 10M input context length for Llama 4 Scout&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[264]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;For mixing modalities, we came up with a carefully curated curriculum strategy that does not trade-off performance compared to the individual modality expert models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[265]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;With Llama 4, we revamped our post-training pipeline by adopting a different approach: lightweight supervised fine-tuning (SFT) &gt; online reinforcement learning (RL) &gt; lightweight direct preference optimization (DPO)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[266]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;A key learning was that SFT and DPO can over-constrain the model, restricting exploration during the online RL stage and leading to suboptimal accuracy, particularly in reasoning, coding, and math domains&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[267]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;To address this, we removed more than 50% of our data tagged as easy by using Llama models as a judge and did lightweight SFT on the remaining harder set&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[268]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;In the subsequent multimodal online RL stage, by carefully selecting harder prompts, we were able to achieve a step change in performance&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[269]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Furthermore, we implemented a continuous online RL strategy, where we alternated between training the model and then using it to continually filter and retain only medium-to-hard difficulty prompts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[270]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This strategy proved highly beneficial in terms of compute and accuracy tradeoffs&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[271]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We then did a lightweight DPO to handle corner cases related to model response quality, effectively achieving a good balance between the model&#x27;s intelligence and conversational abilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[272]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Both the pipeline architecture and the continuous online RL strategy with adaptive data filtering culminated in an industry-leading, general-purpose chat model with state-of-the-art intelligence and image understanding capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[273]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Scout is both pre-trained and post-trained with a 256K context length, which empowers the base model with advanced length generalization capability&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[274]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We present compelling results in tasks such as retrieval with &#x27;retrieval needle in haystack&#x27; for text as well as cumulative negative log-likelihoods (NLLs) over 10 million tokens of code&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[275]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;A key innovation in the Llama 4 architecture is the use of interleaved attention layers <a href="https://arxiv.org/abs/2305.19466" target="_blank">without positional embeddings</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[276]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Additionally, we employ <a href="https://arxiv.org/pdf/2501.19399" target="_blank">inference time temperature scaling</a> of attention to enhance length generalization&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[277]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We call this the iRoPE architecture, where &#x27;i&#x27; stands for &#x27;interleaved&#x27; attention layers, highlighting the long-term goal of supporting &#x27;infinite&#x27; context length, and &#x27;RoPE&#x27; refers to the <a href="https://arxiv.org/abs/2104.09864" target="_blank">rotary position embeddings</a> employed in most layers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[278]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We trained both of our models on a wide variety of image and video frame stills in order to give them broad visual understanding, including of temporal activities and related images&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[279]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This enables effortless interaction on multi-image inputs alongside text prompts for visual reasoning and understanding tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[280]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;The models were pre-trained on up to 48 images, and we&#x27;ve tested in post-training with good results up to eight images&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[281]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Scout is also best-in-class on image grounding, able to align user prompts with relevant visual concepts and anchor model responses to regions in the image&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[282]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This enables more precise visual question answering for the LLM to better understand user intent and localize objects of interest&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[283]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Scout also exceeds comparable models on coding, reasoning, long context, and image benchmarks and offers stronger performance than all previous Llama models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[284]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We&#x27;re excited to share a preview of Llama 4 Behemoth, a teacher model that demonstrates advanced intelligence among models in its class&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[285]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Llama 4 Behemoth is also a multimodal mixture-of-experts model, with 288B active parameters, 16 experts, and nearly two trillion total parameters&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[286]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Offering state-of-the-art performance for non-reasoning models on math, multilinguality, and image benchmarks, it was the perfect choice to teach the smaller Llama 4 models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[287]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We codistilled the Llama 4 Maverick model from Llama 4 Behemoth as a teacher model, resulting in substantial quality improvements across end task evaluation metrics&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[288]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We developed a novel distillation loss function that dynamically weights the soft and hard targets through training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[289]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Codistillation from Llama 4 Behemoth during pre-training amortizes the computational cost of resource-intensive forward passes needed to compute the targets for distillation for the majority of the training data used in student training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[290]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;For additional new data incorporated in student training, we ran forward passes on the Behemoth model to create distillation targets&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[291]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Post-training a model with two trillion parameters was a significant challenge too that required us to completely overhaul and revamp the recipe, starting from the scale of data&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[292]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;In order to maximize performance, we had to prune 95% of the SFT data, as opposed to 50% for smaller models, to achieve the necessary focus on quality and efficiency&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[293]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We also found that doing lightweight SFT followed by large-scale reinforcement learning (RL) produced even more significant improvements in reasoning and coding abilities of the model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[294]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Our RL recipe focused on sampling hard prompts by doing pass@k analysis with the policy model and crafting a training curriculum of increasing prompt hardness&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[295]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We also found that dynamically filtering out prompts with zero advantage during training and constructing training batches with mixed prompts from multiple capabilities were instrumental in providing a performance boost on math, reasoning, and coding&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[296]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Finally, sampling from a variety of system instructions was crucial in ensuring that the model retained its instruction following ability for reasoning and coding and was able to perform well across a variety of tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[297]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Scaling RL for a two trillion parameter model also required revamping our underlying RL infrastructure due to its unprecedented scale&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[298]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We optimized the design of our MoE parallelization for speed, which enabled faster iteration&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[299]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;We developed a fully asynchronous online RL training framework that enhanced flexibility&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[300]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;Compared to the existing distributed training framework, which sacrifices the compute memory in order to stack all models in memory, our new infrastructure enabled flexible allocation of different models to separate GPUs, balancing resources across multiple models based on computational speed&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[301]</span>
<div class="reference-content">
<div class="reference-citation">Meta AI. &quot;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.&quot; Meta AI Blog, April 5, 2025. https://ai.meta.com/blog/llama-4-multimodal-intelligence/</div>
<div class="reference-quote">&quot;This innovation resulted in a ~10x improvement in training efficiency over previous generations&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[302]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek. &quot;DeepSeek V3.&quot; DeepSeek Blog, 2025. https://deepseek.com/blog/DeepSeek-V3</div>
<div class="reference-quote">&quot;自研训练框架、自建智算集群和万卡算力&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[303]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek. &quot;DeepSeek V3.&quot; DeepSeek Blog, 2025. https://deepseek.com/blog/DeepSeek-V3</div>
<div class="reference-quote">&quot;仅用半年时间便已发布并开源多个百亿级参数大模型&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[304]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[305]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[306]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[307]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;However, it encounters challenges such as poor readability, and language mixing&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[308]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[309]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[310]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;DeepSeek-R1 achieves a score of $7 9 . 8 %$ Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[311]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;On MATH-500, it attains an impressive score of $9 7 . 3 %$ , performing on par with OpenAI-o1-1217 and significantly outperforming other models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[312]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming $9 6 . 3 %$ human participants in the competition&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[313]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeekR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of $9 0 . 8 %$ on MMLU, $8 4 . 0 %$ on MMLU-Pro, and $7 1 . 5 %$ on GPQA Diamond&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[314]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[315]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[316]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[317]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;DeepSeekR1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[318]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through ${ \\\\mathrm { R L } } ,$ without the need for SFT&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[319]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[320]</span>
<div class="reference-content">
<div class="reference-citation">DeepSeek-AI. &quot;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.&quot; arXiv, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[321]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Group Relative Policy Optimization (GRPO) is introduced in DeepSeekMath&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[322]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;GRPO can be viewed as PPO-inspired algorithm with a very similar surrogate loss, but it avoids learning a value function with another copy of the original policy language model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[323]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Avoiding the challenge of learning a value function from a LM backbone&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[324]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Saves memory by not needing to keep another set of model weights in memory&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[325]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In order to save the training costs of ${ \\\\scriptstyle \\\\mathrm { R L } } ,$ , we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[326]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Specifically, for each question $q ,$ , GRPO samples a group of outputs ${ o \\_ { 1 } , o \\_ { 2 } , \\\\cdots , o \\_ { G } }$ from the old policy $\\\\pi \\_ { \\\\theta \\_ { o l d } }$ and then optimizes the policy model $\\\\scriptstyle { \\\\pi \\_ { \\\\theta } }$ by maximizing the following objective&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[327]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The reward is the source of the training signal, which decides the optimization direction of RL&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[328]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: Accuracy rewards: The accuracy reward model evaluates whether the response is correct&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[329]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[330]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[331]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between &#x27;&#x27; and $&#x27; \\_ { &lt; }$ /think&gt;&#x27; tags&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[332]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[333]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[334]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This template requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[335]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strategies—to ensure that we can accurately observe the model&#x27;s natural progression during the RL process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[336]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Figure 2 depicts the performance trajectory of DeepSeekR1-Zero on the AIME 2024 benchmark throughout the RL training process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[337]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the RL training advances&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[338]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Notably, the average pass $@ 1$ score on AIME 2024 shows a significant increase, jumping from an initial $1 5 . 6 %$ to an impressive $7 1 . 0 %$ , reaching performance levels comparable to OpenAI-o1-0912&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[339]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This significant improvement highlights the efficacy of our RL algorithm in optimizing the model&#x27;s performance over time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[340]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The findings reveal that RL empowers DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised fine-tuning data&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[341]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This is a noteworthy achievement, as it underscores the model&#x27;s ability to learn and generalize effectively through RL alone&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[342]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Additionally, the performance of DeepSeekR1-Zero can be further augmented through the application of majority voting&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[343]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For example, when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero&#x27;s performance escalates from $7 1 . 0 %$ to $8 6 . 7 %$ , thereby exceeding the performance of OpenAI-o1-0912&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[344]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[345]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The self-evolution process of DeepSeek-R1-Zero is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[346]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;By initiating RL directly from the base model, we can closely monitor the model&#x27;s progression without the influence of the supervised fine-tuning stage&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[347]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[348]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement throughout the training process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[349]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This improvement is not the result of external adjustments but rather an intrinsic development within the model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[350]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;DeepSeek-R1-Zero naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[351]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[352]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;One of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[353]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[354]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;These behaviors are not explicitly programmed but instead emerge as a result of the model&#x27;s interaction with the reinforcement learning environment&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[355]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This spontaneous development significantly enhances DeepSeek-R1-Zero&#x27;s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[356]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;A particularly intriguing phenomenon observed during the training of DeepSeek-R1-Zero is the occurrence of an &#x27;aha moment&#x27;&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[357]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This moment, as illustrated in Table 3, occurs in an intermediate version of the model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[358]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;During this phase, DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial approach&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[359]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This behavior is not only a testament to the model&#x27;s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[360]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This moment is not only an &#x27;aha moment&#x27; for the model but also for the researchers observing its behavior&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[361]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[362]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The &#x27;aha moment&#x27; serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[363]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Although DeepSeek-R1-Zero exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[364]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For instance, DeepSeek-R1-Zero struggles with challenges like poor readability, and language mixing&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[365]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-start data&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[366]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[367]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To address these questions, we design a pipeline to train DeepSeek-R1&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[368]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;The pipeline consists of four stages, outlined as follows&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[369]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1- Zero outputs in a readable format, and refining the results through post-processing by human annotators&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[370]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Compared to DeepSeek-R1-Zero, the advantages of cold start data include: Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[371]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Responses may mix multiple languages or lack markdown formatting to highlight answers for users&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[372]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[373]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Here, we define the output format as \\|special\\_token\\|\\|special\\_token\\|, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[374]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Potential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against DeepSeek-R1-Zero&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[375]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We believe the iterative training is a better way for reasoning models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[376]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[377]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;This phase focuses on enhancing the model&#x27;s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[378]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[379]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[380]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Although ablation experiments show that such alignment results in a slight degradation in the model&#x27;s performance, this reward aligns with human preferences, making it more readable&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[381]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[382]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[383]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[384]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model&#x27;s capabilities in writing, role-playing, and other general-purpose tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[385]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Specifically, we generate the data and fine-tune the model as described below&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[386]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Reasoning data We curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[387]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In the previous stage, we only included data that could be evaluated using rule-based rewards&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[388]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[389]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[390]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For each prompt, we sample multiple responses and retain only the correct ones&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[391]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In total, we collect about 600k reasoning related training samples&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[392]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[393]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[394]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;However, for simpler queries, such as &#x27;hello&#x27; we do not provide a CoT in response&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[395]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[396]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[397]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;To further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model&#x27;s helpfulness and harmlessness while simultaneously refining its reasoning capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[398]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Specifically, we train the model using a combination of reward signals and diverse prompt distributions&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[399]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[400]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[401]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[402]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[403]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[404]</span>
<div class="reference-content">
<div class="reference-citation">D. Guo _et al._, &quot;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,&quot; _arXiv preprint arXiv:2501.12948_, 2025. https://arxiv.org/pdf/2501.12948</div>
<div class="reference-quote">&quot;Ultimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[405]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Reinforcement learning from Human Feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[406]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The basic pipeline for RLHF involves three steps. First, a language model that can follow user questions must be trained&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[407]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Second, human preference data must be collected for the training of a reward model of human preferences&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[408]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Finally, the language model can be optimized with an RL optimizer of choice, by sampling generations and rating them with respect to the reward model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[409]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;RLHF has been applied to many domains successfully, with complexity increasing as the techniques have matured&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[410]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;In modern language model training, RLHF is one component of post-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[411]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Post-training is a more complete set of techniques and best-practices to make language models more useful for downstream tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[412]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Post-training can be summarized as using three optimization methods: Instruction / Supervised Finetuning (IFT/SFT), Preference Finetuning (PreFT), and Reinforcement Finetuning (RFT)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[413]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and form the base of instruction following abilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[414]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Preference Finetuning (PreFT), where we align to human preferences (and get smaller bump in capabilities at the same time)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[415]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Reinforcement Finetuning (RFT). The newest type of post-training that boosts performance on verifiable domains&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[416]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;This book focuses on the second area, preference finetuning, which has more complexity than instruction tuning and is far more established than Reinforcement Finetuning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[417]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;RLHF colloquially is what led to modern post-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[418]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The core role of this book, beyond teaching the techniques for doing RLHF, is to distill intuition as to why RLHF is crucial to modern AI models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[419]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Modern research has established RLHF as a general method to integrate subtle stylistic and related behavioral features into the models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[420]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Compared to other techniques for post-training, such as instruction finetuning, RLHF generalizes far better across domains&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[421]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Instruction finetuning is training the model to predict the next certain token when the text preceding is close to examples it has seen&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[422]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;RLHF on the other hand tunes the responses on the response level rather than looking at the next token specifically&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[423]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;RLHF also shows a model which type of response it should avoid, i.e. negative feedback&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[424]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The training to achieve this is often called a contrastive loss function and is referenced throughout this book&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[425]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;While this flexibility is a major advantage of RLHF, it comes with implementation challenges&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[426]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Largely, these center on how to control the optimization&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[427]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Implementing RLHF often requires training a reward model, of which best practices are not strongly established and depend on the area of application&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[428]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The optimization itself is prone to over-optimization because our reward signal is at best a proxy objective, requiring regularization&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[429]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Effective RLHF requires a strong starting point, so RLHF cannot be a solution to every problem alone and needs to be approached in a broader lens of post-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[430]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;Due to this complexity, implementing RLHF is far more costly than simple instruction finetuning and can come with unexpected challenges such as length bias&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[431]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;For projects where performance matters, RLHF is established as being crucial to achieving a strong finetuned model, but it is more expensive in compute, data costs, and time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[432]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The intuition I&#x27;ve been using to understand the potential of post-training is called the elicitation interpretation of post-training, where all we are doing is extracting and amplifying valuable behaviors in the base model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[433]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The best post-training teams extract a ton of performance in a very short time frame&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[434]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;The set of techniques is everything after the end of most of pretraining&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[435]</span>
<div class="reference-content">
<div class="reference-citation">N. Lambert. &quot;Reinforcement Learning from Human Feedback.&quot; arXiv preprint arXiv:2504.12501v3, November 2, 2025. https://arxiv.org/html/2504.12501v3</div>
<div class="reference-quote">&quot;This theory folds in with the reality that the majority of gains users are seeing are from post-training because it implies that there is more latent potential in a model pretraining on the internet than we can teach the model simply&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[436]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;GPT-5 used less training compute than GPT-4.5 because OpenAI focused on scaling post-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[437]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;New post-training techniques made it possible to outperform GPT-4.5 with less training compute, but these methods likely weren&#x27;t yet mature enough to be applied at GPT-4.5&#x27;s compute scale&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[438]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Doing so would&#x27;ve taken more time (and compute), which OpenAI likely chose not to do due to strong market pressures&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[439]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Until recently, most LLMs were trained with <a href="https://epoch.ai/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining" target="_blank">100× more pre-training than post-training compute</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[440]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;However, around September 2024, researchers developed novel techniques used in &#x27;reasoning models&#x27; that help scale post-training compute effectively&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[441]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Researchers could now triple post-training compute in a way that was at least as useful as tripling pre-training compute&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[442]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;In fact, these reasoning techniques make it possible to reduce pre-training compute by <a href="https://epoch.ai/gradient-updates/quantifying-the-algorithmic-improvement-from-reasoning-models" target="_blank">roughly 10×</a> while getting the same performance!&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[443]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Out of all the GPT models, GPT-5 is the odd one out&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[444]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Unlike all previous versions of GPT, it was likely <a href="https://x.com/EpochAIResearch/status/1953883611389702169" target="_blank">trained on less compute</a> than its immediate predecessor, GPT-4.5&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[445]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;While the exact numbers are uncertain, GPT-4.5 very likely used more training compute than GPT-5&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[446]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;But this leads to a puzzle: Models trained with more compute tend to be better, so why did OpenAI train GPT-5 with less compute than GPT-4.5?&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[447]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Importantly, when we say &#x27;training compute&#x27;, we&#x27;re focusing on the compute to perform the final training run of a model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[448]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;It&#x27;s likely that the total compute for developing GPT-5 was higher than for GPT-4.5, if we also account for the compute for running experiments&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[449]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;This is because OpenAI&#x27;s (projected) R&amp;D compute spend has grown from <a href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=spkbjw" target="_blank">~$5 billion</a> in 2024 to <a href="https://www.theinformation.com/articles/openai-spend-100-billion-backup-servers-ai-breakthroughs?rc=spkbjw" target="_blank">~$9 billion in 2025</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[450]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Why did GPT-5 use less training compute than GPT-4.5? We believe this is a combination of two factors&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[451]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;First, OpenAI decided to prioritize scaling post-training, which had better returns on the margin&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[452]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Second, they couldn&#x27;t readily scale post-training compute to GPT-4.5 levels at the time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[453]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;And if they tried to scale post-training on a model with as much pre-training as GPT-4.5, they would&#x27;ve run into timing and experimental compute constraints&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[454]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;This means that, rather than spending around $200 million on pre-training and $2 million on post-training GPT-4.5, new post-training techniques made it possible for that $2 million in post-training to achieve the same overall performance with only $20 million in pre-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[455]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;That&#x27;s roughly a ten-fold decrease in training costs, though this doesn&#x27;t imply that total model development costs were lower, due to increases in the compute needed to run experiments&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[456]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;The upshot is that OpenAI was likely able to train a model with less compute than GPT-4.5, while still <a href="https://openai.com/index/introducing-gpt-4-5/" target="_blank">outperforming</a> it on many useful tasks like <a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf" target="_blank">coding and search</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[457]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;However, while this shows that OpenAI could&#x27;ve outperformed GPT-4.5 with less training compute, it doesn&#x27;t fully explain why they chose this strategy in practice&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[458]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;For example, why not just post-train GPT-4.5? And why not post-train a smaller model on enough data to reach GPT-4.5&#x27;s level of training compute?&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[459]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;The core reason is that scaling post-training in this way is challenging&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[460]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;It requires lots of testing and experimentation, which takes time and compute, especially when performed on larger, newer models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[461]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;It also requires a significant amount of high-quality post-training data, which takes time to design and collect&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[462]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Crucially, OpenAI faced major time constraints due to market pressures&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[463]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;This came in the form of fierce competition from rival AI labs, which would hurt their <a href="https://epoch.ai/data-insights/ai-companies-revenue" target="_blank">revenue</a> – e.g. Anthropic&#x27;s models had been <a href="https://epoch.ai/benchmarks/swe-bench-verified" target="_blank">consistently outperforming</a> OpenAI&#x27;s models at coding&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[464]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;And there was added pressure because many had expected OpenAI to release a model called &#x27;GPT-5&#x27; as early as <a href="https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded" target="_blank">November 2023</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[465]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Given these constraints, we believe that OpenAI scaled post-training on a smaller model as much as they could&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[466]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Scaling further would&#x27;ve either required more experiments than they had the compute or time for, or post-training data that they didn&#x27;t have&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[467]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Post-training a GPT-4.5-sized model, let alone starting a larger <a href="https://epoch.ai/data-insights/longest-training-run" target="_blank">multi-month</a> pre-training run and doing post-training on top, would&#x27;ve taken too much time or too much experiment compute&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[468]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;The result of these efforts in scaling post-training was GPT-5, a new state-of-the-art model that OpenAI was able to release by August&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[469]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;What does this mean for training compute trends moving forward? Our best guess is that future iterations of GPT will be trained on more compute&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[470]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;To see why, consider the bigger picture&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[471]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Training GPT-5 with less compute than GPT-4.5 is part of a broader trend, where the training compute of state-of-the-art models has grown more slowly than one might&#x27;ve expected a year ago&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[472]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Since post-training was just a small portion of training compute and scaling it yielded huge returns, AI labs focused their limited training compute on scaling it rather than pre-training&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[473]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;In fact, these reasoning post-training techniques have scaled <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale" target="_blank">much faster than pre-training compute</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[474]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;At this rate, tripling post-training compute will soon be akin to tripling the entire compute budget – so current growth rates likely <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale" target="_blank">can&#x27;t be sustained</a> for much more than a year&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[475]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;That means that this broader trend is likely to end – we may see a reversion to the original trend of training compute growth&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[476]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;If this is right, GPT-6 is likely to need much more training compute than GPT-5, and probably more than GPT-4.5&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[477]</span>
<div class="reference-content">
<div class="reference-citation">Epoch AI. &quot;Why GPT-5 used less training compute than GPT-4.5 (but GPT-6 probably won&#x27;t).&quot; Epoch AI, September 26, 2025. https://epoch.ai/gradient-updates/why-gpt5-used-less-training-compute-than-gpt45-but-gpt6-probably-wont</div>
<div class="reference-quote">&quot;Not to mention, OpenAI plans to significantly expand their compute stock, with many more <a href="https://x.com/sama/status/1947057625780396512?t=OwQjfzYJTdYzy8vRCXaJHg" target="_blank">GPUs brought online by the end of the year</a>, and major clusters like <a href="https://openai.com/index/five-new-stargate-sites/" target="_blank">Stargate Abilene</a> coming out in phases&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[478]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Today, OpenAI previewed their o3 model continuing their recent progress on training language models to reason with o1&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[479]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;These models, starting with o3-mini, are expected to be available to the general public in late January of 2025&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[480]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;There was no moment with a &#x27;<a href="https://www.interconnects.ai/p/gpt4-review" target="_blank">GPT-4 release</a>&#x27; level of excitement in 2024&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[481]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;o3 changes that by being far more unexpected than o1, and signals rapid progress across reasoning models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[482]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;We knew o1 was coming with the long lead-up — the quick and effective follow-up with o3 sets us up for a very dynamic 2025&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[483]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;OpenAI&#x27;s o3 shows the industry is beginning to climb its next hill as progress from pretraining only on internet text yields fewer profitable benefits&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[484]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;o3 is a major step change in reasoning evaluations — in summary, it is: The first model to <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough" target="_blank">surpass the 85% threshold for completing the ARC AGI prize</a> (Note: this was done on the public set, not the test set, and exceeded cost constraints)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[485]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;A step change in state-of-the-art performance on the extremely new <a href="https://epoch.ai/frontiermath" target="_blank">Frontier Math</a> benchmark from 2 to 25%&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[486]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Substantial improvements were made to all of the leading coding benchmarks, such as SWE-Bench-Verified&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[487]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Before the o1-class models, OpenAI&#x27;s best model, GPT-4o, only achieved 5% accuracy&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[488]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;The incredible pace of progress on the evaluation as OpenAI hillclimbed on their new reasoning models was <a href="https://x.com/mikeknoop/status/1870172132136931512" target="_blank">summarized by co-founder of ARC Prize Mike Knoop</a>: GPT-2 (2019): 0%, GPT-3 (2020): 0%, GPT-4 (2023): 2%, GPT-4o (2024): 5%, o1-preview (2024): 21%, o1 high (2024): 32%, o1 Pro (2024): ~50%, o3 tuned low (2024): 76%, o3 tuned high (2024): 87%&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[489]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Just in June, the narrative was still that <a href="https://x.com/fchollet/status/1804236584432398622" target="_blank">solving ARC-AGI would be extremely hard</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[490]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;This has totally flipped on its head in just a few months&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[491]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Even those bullish about rumors of Q* and other reasoning approaches would not have expected this level of success&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[492]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;We tested o3 against two ARC-AGI datasets: Semi-Private Eval: 100 private tasks used to assess overfitting, Public Eval: 400 public tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[493]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;At OpenAI&#x27;s direction, we tested at two levels of compute with variable <strong>sample sizes</strong>: 6 (high-efficiency) and 1024 (low-efficiency, 172x compute)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[494]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;According to <a href="https://semianalysis.com/2024/12/11/scaling-laws-o1-pro-architecture-reasoning-training-infrastructure-orion-and-claude-3-5-opus-failures/" target="_blank">SemiAnalysis</a>, o1 pro uses self-consistency methods or simple consensus@N checks to increase performance by selecting the most common answer across multiple parallel responses to the same query&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[495]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Here, <strong>sample size</strong>, N, likely corresponds to a consensus@N number, indicating that o3 was evaluated in something close to the configuration for o1 pro that customers can use, 6x compute, and a super high configuration with 1024x compute per problem&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[496]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;This scale of inference is not going to be served to standard paid users for a long time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[497]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Most users will be exposed to one pass to consensus@10 depending on the specifications of the &#x27;pro&#x27; tier of o1 models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[498]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;The story in deep learning that has been driving progress in the last few years is finding a rich area of potential and hill climbing on it&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[499]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;The first wave of progress was in internet-scale pretraining&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[500]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Now, OpenAI has identified a hill to climb by scaling reinforcement learning training and long-context reasoning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[501]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;Given that o3 is only about <a href="https://x.com/_jasonwei/status/1870184982007644614" target="_blank">three months</a> after the <a href="https://www.interconnects.ai/p/reverse-engineering-openai-o1" target="_blank">release of OpenAI&#x27;s o1</a>, the simplest explanation is that it is the same architecture and training methodology, scaled up&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[502]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;There is no evidence other than hearsay that o3 made an architectural change to inference by adding tree search&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[503]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;A core rule of <a href="https://www.interconnects.ai/i/148388607/inference-scaling-laws" target="_blank">inference scaling laws</a> is that sampling more from the same single-stream generation can give performance improvements&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[504]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;At the same time, OpenAI released a <a href="https://openai.com/index/deliberative-alignment/" target="_blank">blog post</a> and research <a href="https://assets.ctfassets.net/kftzwdyauwt9/4pNYAZteAQXWtloDdANQ7L/32db347774f3b8e43a028cc98b24e416/OpenAI_Deliberative-Alignment-Reasoning-Enables-Safer_Language-Models_122024_2.pdf" target="_blank">paper</a> on deliberative alignment, showcasing how o1-class models can enhance safety and alignment research&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[505]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;This provides some of the first positive pieces of evidence for the much bigger open question I hinted at earlier: Can enhanced reasoning abilities deliver value outside of verifiable domains?&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[506]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;OpenAI o3: Reasoning Models Scale.&quot; Interconnects, December 2024. https://www.interconnects.ai/p/openai-o3</div>
<div class="reference-quote">&quot;This will be revisited many times in 2025&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[507]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm to improve the reasoning capabilities of LLMs&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[508]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;It was introduced in the <a href="https://arxiv.org/abs/2402.03300" target="_blank">DeepSeekMath</a> paper in the context of mathematical reasoning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[509]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;GRPO modifies the traditional Proximal Policy Optimization (PPO) by eliminating the need for a value function model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[510]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Instead, it estimates baselines from group scores, reducing memory usage and computational overhead&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[511]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;GRPO, now also used by the Qwen team, can be used with rule/binary-based Rewards as well as General Reward Models to improve models on helpfulness&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[512]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Starting with DeepSeek V3, they applied GRPO to unsupervised reasoning text completions rule-based reward models that focused on aspects like format, mathematics, and coding&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[513]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Accuracy rewards: Evaluate whether the response is correct, correct result or compiled LeetCode problem&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[514]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Format rewards: Evaluate the format that enforces the model to put its thinking process between &#x27;&#x27; and &#x27;&#x27; tags&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[515]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;This leads to a pass@1 score on AIME 2024 increasing from 15.6% to 71.0%, reaching performance levels comparable to OpenAI-o1-0912 alongside output token length per problem increasing, indicating the model naturally learns to solve tasks with more thinking time/token generation&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[516]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;This has the drawback of leading to poor readability and language mixing but it was solved for R1 using a multi-stage approach with alternating SFT → RL steps&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[517]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;To prevent the early unstable cold start phase of reinforcement training (RL) training from the base model, the team started with supervised fine-tuning&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[518]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Collected up to 10k token-long chain-of-thought (CoT) using the fine-tuned models, R1-zero and human annotator&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[519]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;The data is used to fine-tune Deepseek V3 base to improve readbility and coherence&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[520]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Used the same RL pipeline as R1-Zero, focusing on reasoning-intensive tasks such as coding and math using the same Rule-Based Reward Models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[521]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;This time, an additional reward for &#x27;language consistency&#x27; is used to help the model stick to the same language&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[522]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Generated large synthetic dataset using Reject Sampling (RS) focusing on writing, role-playing, and other general-purpose tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[523]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;The model from Stage 2 was used with Deepseek V3 as a Judge to generate 600k reasoning-related samples and 200k for writing, role-playing, and other general-purpose tasks using portions of the SFT dataset of DeepSeek-V3 or regenerating them with CoT included&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[524]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;In the Final Stage, GRPO is used again with a combination of Rule-Based and Outcome Reward Models to improve the model&#x27;s helpfulness and harmlessness&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[525]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Leading to the <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" target="_blank">Deepseek R1</a> model&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[526]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;DeepSeek didn&#x27;t use Monte Carlo Tree Search (MCTS) or Process Reward Models (PRM)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[527]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Fine-tuning before applying GRPO can actually make the training process faster and more stable&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[528]</span>
<div class="reference-content">
<div class="reference-citation">Interconnects. &quot;DeepSeek R1 Training Recipe.&quot; Interconnects, 2025. https://www.interconnects.ai/p/deepseek-r1-training-recipe</div>
<div class="reference-quote">&quot;Rule-based rewards focused on accuracy and format are more effective than complex rewards models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[529]</span>
<div class="reference-content">
<div class="reference-citation">Baytech Consulting. &quot;Claude AI 2025.&quot; Baytech Consulting Blog, 2025. https://www.baytechconsulting.com/blog/claude-ai-2025</div>
<div class="reference-quote">&quot;Claude: Employs Constitutional AI (CAI) with RLAIF, using AI-generated feedback based on explicit principles to guide behavior towards being&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[530]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Google AI Ultra subscribers, you now have access to Deep Think in the Gemini app&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[531]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;This tool uses parallel thinking to solve complex problems and excels in areas like coding and scientific discovery&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[532]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;You can access Deep Think by toggling it on in the prompt bar within the Gemini app&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[533]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;This new release incorporates feedback from early trusted testers and research breakthroughs&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[534]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;It&#x27;s a significant improvement over what was first <a href="https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#deep-think" target="_blank">announced at I/O</a>, as measured in terms of key benchmark improvements and trusted tester feedback&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[535]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;It is a variation of the model that <a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/" target="_blank">recently achieved</a> the gold-medal standard at this year&#x27;s International Mathematical Olympiad (IMO)&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[536]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;While that model takes hours to reason about complex math problems, today&#x27;s release is faster and more usable day-to-day, while still reaching Bronze-level performance on the 2025 IMO benchmark, based on internal evaluations&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[537]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Deep Think pushes the frontier of thinking capabilities by using parallel thinking techniques&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[538]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;This approach lets Gemini generate many ideas at once and consider them simultaneously, even revising or combining different ideas over time, before arriving at the best answer&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[539]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Moreover, by extending the inference time or &#x27;thinking time,&#x27; we give Gemini more time to explore different hypotheses, and arrive at creative solutions to complex problems&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[540]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;We&#x27;ve also developed novel reinforcement learning techniques that encourage the model to make use of these extended reasoning paths, thus enabling Deep Think to become a better, more intuitive problem-solver over time&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[541]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Deep Think&#x27;s performance is also reflected in challenging benchmarks that measure coding, science, knowledge and reasoning capabilities&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[542]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;For example, compared to other models without tool use, Gemini 2.5 Deep Think achieves state-of-the-art performance across LiveCodeBench V6, which measures competitive code performance, and Humanity&#x27;s Last Exam, a challenging benchmark that measures expertise in different domains, including science and math&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[543]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Deep Think in the Gemini app uses parallel thinking techniques to deliver more detailed, creative and thoughtful responses&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[544]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Deep Think can improve both the aesthetics and functionality of web development tasks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[545]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Algorithmic development and code&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[546]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;We continue to build safety and responsibility into Gemini throughout the training and deployment lifecycle&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[547]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;In testing, Gemini 2.5 Deep Think demonstrated improved content safety and tone-objectivity compared to Gemini 2.5 Pro, but did have a higher tendency to refuse benign requests&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[548]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;As Gemini&#x27;s problem-solving abilities advance, we are taking a deeper look at risks that come with increased complexity, including our frontier safety evaluations and the implementation of planned mitigations for critical capability levels&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[549]</span>
<div class="reference-content">
<div class="reference-citation">Google. &quot;Try Deep Think in the Gemini app.&quot; Google Blog, August 1, 2025. https://blog.google/products/gemini/gemini-2-5-deep-think/</div>
<div class="reference-quote">&quot;Further details on the safety outcomes of Gemini 2.5 Deep Think are available in the <a href="https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf" target="_blank">model card</a>&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[550]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">the single hardest topic to research and write about on the web today</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[551]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">exact training datasets and data mixture ratios, token counts, preprocessing pipelines, compute budgets and cluster topologies, optimizer configurations, RLHF methodologies, supervised fine-tuning sources, architecture modifications (MoE routing logic, attention variants, parallelism strategies), internal eval benchmarks, safety red-team results, and alignment techniques</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[552]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[553]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">evolves so fast that even partial information becomes outdated within weeks, and widespread speculation, leaks, and misinformation drown out credible analysis&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[554]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware—accurate reporting requires deep multi-domain expertise yet still lacks access to the primary evidence needed for certainty&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[555]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[556]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">evolves so fast that even partial information becomes outdated within weeks, and widespread speculation, leaks, and misinformation drown out credible analysis&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[557]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware—accurate reporting requires deep multi-domain expertise yet still lacks access to the primary evidence needed for certainty&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[558]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">internal eval benchmarks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[559]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">These benchmarks are used to assess model capabilities before public release, though specific details remain proprietary&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[560]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Constitutional AI, Deliberative Alignment, RLAIF&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[561]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Specific implementation details remain proprietary, though research indicates these techniques are critical for ensuring model safety and alignment with human values&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[562]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Key technical details are intentionally undisclosed or obscured&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[563]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">These details are hidden due to competitive, economic, and geopolitical pressure, while public statements are often vague marketing layers rather than technical truth&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[564]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">evolves so fast that even partial information becomes outdated within weeks&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[565]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">This rapid evolution means that research findings may become obsolete quickly, requiring constant updates and verification&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[566]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Independent verification is nearly impossible—training runs cost millions and rely on restricted hardware&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[567]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">This makes it difficult to verify claims about model capabilities, training methods, and performance&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[568]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Widespread speculation, leaks, and misinformation drown out credible analysis&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[569]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">This creates challenges in distinguishing accurate information from speculation or misinformation&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[570]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">The relationship between compute, model size, dataset size, and performance&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[571]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">The use of larger models to generate data for training smaller models&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[572]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">This approach could potentially reduce training costs while maintaining performance&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[573]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">attention mechanisms&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[574]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">FlashAttention, Slim Attention, Scalable Softmax, Hyena, Mamba, RWKV, RetNet, and Differential Transformers&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[575]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">data parallelism, model parallelism, tensor parallelism, pipeline parallelism, and hybrid parallelism&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[576]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Researching frontier AI model capabilities and training methods presents significant challenges due to intentional opacity, rapid evolution, and verification difficulties&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[577]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">While some information is available through official announcements, research papers, and technical reports, many critical details remain proprietary&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[578]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">The field continues to evolve rapidly, with new models and techniques emerging regularly&quot;</div>
</div>
</div>

<div class="reference-item">
<span class="reference-number">[579]</span>
<div class="reference-content">
<div class="reference-citation">Analysis based on research findings</div>
<div class="reference-analysis">Understanding these models requires careful analysis of available information while acknowledging the limitations imposed by competitive secrecy and proprietary development practices&quot;</div>
</div>
</div>


<hr>

<p><strong>Document Status:</strong> Document expanded with exact quotes from multiple sources. Currently includes 579 references. All statements in this document are direct quotes from source materials as required by the research workflow configuration. All references include their direct quotes in the references section as requested.</p>

</div>

        </div>
        <div class="footer">
            <p>Research Summary - Frontier AI Model Capabilities and Training Methods</p>
            <p>Generated: November 14, 2025</p>
        </div>
    </div>
</body>
</html>
